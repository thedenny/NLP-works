{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好我是771号,请问你要耍一耍喝酒吗？'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "def get_generation_by_gram(gram_str: str, target, stmt_spilt='=', or_split='|'):\n",
    "    \n",
    "    rules=dict()    ##将gram_str信息存入字典rules（stmt&expr）\n",
    "    \n",
    "    for line in gram_str.split('\\n'):\n",
    "        if not line:    continue\n",
    "        stmt, expr = line.split(stmt_spilt)\n",
    "        rules[stmt.strip()] = expr.split(or_split) \n",
    "            \n",
    "    generated = generate(rules, target=target)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def generate(gram_rule,target):\n",
    "    \n",
    "    if target in gram_rule:     ##names\n",
    "        candidates = gram_rule[target]    ##name\n",
    "        candidate = random.choice(candidates)\n",
    "             \n",
    "        \n",
    "        return ''.join(generate(gram_rule, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target\n",
    "    \n",
    "get_generation_by_gram(host, target='host', stmt_spilt='=', or_split='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.734040140230803e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "corpus = 'E:\\\\NLP\\\\Works\\\\1st\\\\article_9k.txt'\n",
    "article_file = open(corpus, encoding='utf-8').read()\n",
    "\n",
    "max_length = 10000\n",
    "sub_file = article_file[:max_length]\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "Tokens = cut(sub_file)\n",
    "word_counts = Counter(Tokens)\n",
    "\n",
    "_2_gram_Tokens = [Tokens[i]+Tokens[i+1] for i in range(len(Tokens)-1)]\n",
    "_2_gram_Tokens_counts = Counter(_2_gram_Tokens)\n",
    "\n",
    "def get_gram_words_counter(word, words_counter):\n",
    "    if word in words_counter:\n",
    "        return words_counter[word]\n",
    "    else:\n",
    "        return words_counter.most_common()[-1][-1]\n",
    "\n",
    "def two_grammar_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    possibility = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = Tokens[i+1]\n",
    "        _1_gram_counts = get_gram_words_counter(tokens[i], word_counts)\n",
    "        _2_gram_counts = get_gram_words_counter(tokens[i]+tokens[i+1], _2_gram_Tokens_counts)\n",
    "        pro = _2_gram_counts/_1_gram_counts\n",
    "        \n",
    "        possibility *= pro\n",
    "        \n",
    "    return possibility\n",
    "\n",
    "two_grammar_model('前天早上吃晚饭的时候')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0723045304960567e-07"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "article_site = 'E:\\\\NLP\\\\Works\\\\1st\\\\train_deal.txt'\n",
    "article_file = open(article_site, encoding='utf-8').read()\n",
    "\n",
    "article_max_length = 50000\n",
    "sub_file = article_file[:article_max_length]\n",
    "\n",
    "Token_words = cut(sub_file)\n",
    "one_words_counts = Counter(Token_words)\n",
    "\n",
    "two_grammar_words = [Token_words[i]+Token_words[i+1] for i in range(len(Token_words)-1)]\n",
    "two_words_counts = Counter(two_grammar_words)\n",
    "\n",
    "\n",
    "def get_generation_by_grammar(grammar_str: str, target, stmt_spilt='=', or_split='|'):\n",
    "    \n",
    "    rule_dict = dict()\n",
    "    \n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line:   continue\n",
    "        stmt, expre = line.split(stmt_spilt)\n",
    "        rule_dict[stmt.strip()] = expre.split(or_split)\n",
    "                    \n",
    "    generated_sentance = generate(rule_dict, target=target)\n",
    "    \n",
    "    return generated_sentance\n",
    "\n",
    "def generate(grammer_rule, target):\n",
    "    \n",
    "    if target in grammer_rule:\n",
    "        candidates = grammer_rule[target]\n",
    "        candidate  = random.choice(candidates)\n",
    "        sentance = ''.join(generate(grammer_rule, target=c.strip()) for c in candidate.split())\n",
    "        \n",
    "        return sentance\n",
    "    else:\n",
    "        return target\n",
    "    \n",
    "def generate_n(a):\n",
    "    sen_list = list()\n",
    "    for i in range(a):\n",
    "        sen_list.append(get_generation_by_grammar(host, target='host', stmt_spilt='=', or_split='|'))     \n",
    "    return sen_list\n",
    "    \n",
    "\n",
    "def cut(string_str):\n",
    "    return list(jieba.cut(string_str))\n",
    "\n",
    "def get_grammar_word_counts(word, grammar_sentance):\n",
    "    if word in grammar_sentance:\n",
    "        return grammar_sentance[word]\n",
    "    else:\n",
    "        return grammar_sentance.most_common()[-1][-1]\n",
    "    \n",
    "def two_grammar_model(given_sentance):\n",
    "    token_words = cut(given_sentance)\n",
    "    possibility = 1\n",
    "    \n",
    "    for i in range(len(token_words)-1):\n",
    "        word = token_words[i]\n",
    "        word_next = token_words[i]+token_words[i+1]\n",
    "        one_words_counter = get_grammar_word_counts(word, one_words_counts)\n",
    "        two_words_counter = get_grammar_word_counts(word_next, two_words_counts)\n",
    "        pro = two_words_counter/one_words_counter\n",
    "        possibility *= pro\n",
    "        \n",
    "    return possibility\n",
    "\n",
    "\n",
    "def generate_best(n):\n",
    "    sentance_possibi_list = list()\n",
    "    sentance_list = generate_n(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        possib = two_grammar_model(sentance_list[i])\n",
    "        sentance_possibi_list.append([sentance_list[i], possib])\n",
    "        \n",
    "    sentance_possibi_sort_list = sorted(sentance_possibi_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    #print(sentance_possibi_sort_list)\n",
    "    \n",
    "    return sentance_possibi_sort_list[0]\n",
    "\n",
    "\n",
    "generate_best(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##清洗文本，只剩中文\n",
    "import re\n",
    "input_file = r'E:\\NLP\\Works\\1st\\train.txt'\n",
    "output_file = r'E:\\NLP\\Works\\1st\\train_deal.txt'  #创建新的文件并写入清洗后的文本\n",
    "\n",
    "input_file_file = open(input_file, encoding='utf-8').read()\n",
    "output_str_list = re.findall(r'[\\u4e00-\\u9fa5]',input_file_file)   #只保留中文\n",
    "output_str = ''.join(output_str_list)   #字符串形式\n",
    "\n",
    "output_file_str = open(output_file, 'w', encoding='utf-8')  ##打开清洗后需要写入的文本\n",
    "output_file_str.write(output_str)\n",
    "\n",
    "output_file_str.close\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
