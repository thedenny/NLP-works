{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: icecream in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from icecream) (0.4.1)\n",
      "Requirement already satisfied: executing>=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from icecream) (0.3.2)\n",
      "Requirement already satisfied: pygments>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from icecream) (2.3.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from icecream) (2.0.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from asttokens>=2.0.1->icecream) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install icecream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fit_fuction is: y = 14.45380357327106*x + 5.125350024062581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb9ccda0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAH2BJREFUeJzt3X90FfWd//Hn24AalRopwUKQYq2ytVKhG20t/Vr8GfV0lfq1FHe16rqlVnG19eQUdF08ba1saatH7VapsrbdFn+sQD2rNlLhW6sVNYgCrVKQ1ULCFgQjWqKG8P7+MTfm/pgkQ+6PuffO63FODve+79zMe8jlxeQzn5kxd0dERJJjn7gbEBGR0lLwi4gkjIJfRCRhFPwiIgmj4BcRSRgFv4hIwij4RUQSRsEvIpIwCn4RkYQZEncDYUaMGOHjxo2Luw0RkYqxcuXK1929PsqyZRn848aNo7W1Ne42REQqhpm9FnVZDfWIiCSMgl9EJGEU/CIiCaPgFxFJmAGD38wOM7PlZvaSmf3BzK5K1Yeb2VIzW5/685A+3n9Rapn1ZnZRoTdARET2TpRZPbuBa9z9eTMbBqw0s6XAxcDj7j7XzGYBs4Bvpr/RzIYDc4BGwFPvfcjd3yjkRohIeVmyqo15Leto7+hkdF0tzU3jmTqpIe62JGXAPX533+Luz6cevwW8BDQA5wA/TS32U2BqyNubgKXuviMV9kuBMwrRuIiUpyWr2pi9aA1tHZ040NbRyexFa1iyqi3u1iRlr8b4zWwcMAl4BjjU3bdA8J8DMDLkLQ3AprTnm1M1EalS81rW0dnVnVHr7OpmXsu6mDqSbJGD38wOAh4Ernb3nVHfFlILvcmvmc0ws1Yza922bVvUtkSkzLR3dO5VXUovUvCb2VCC0P+Fuy9Klf9iZqNSr48Ctoa8dTNwWNrzMUB72Drcfb67N7p7Y319pLOORaQMja6r3au6lF6UWT0G3A285O4/THvpIaBnls5FwK9C3t4CnG5mh6Rm/ZyeqolIlWpuGk/t0JqMWu3QGpqbxsfUUfEtWdXG5LnLOHzWw0yeu6zsj2dEmdUzGbgQWGNmL6Rq1wJzgfvN7FLgz8AXAcysEbjM3f/J3XeY2beB51Lv+5a77yjoFohIWemZvZOUWT09B7N7jmv0HMwGynabzT10yD1WjY2Nrou0iUglmDx3GW0hxy8a6mp5atbJJevDzFa6e2OUZXXmrohIHirxYLaCX0QkD5V4MFvBLyKSh8EezF6yqo3jrn2Shkt/W/IDwmV5IxYRkUoxmIPZ9zy+hUv/rp49ncEyQ5ofLukBYQW/iBRUEq/TM3VSQ6Rt3LEDJk6ETZtGvV/74JkvYvv0nt2s4BeRilKJUxtLYedO+NSn4OWXe2uHnPRHPnD8/2QsV6oDwhrjF5GC0XV6Mu3aBccdBwcf3Bv63/oWfOamZTmhD6U7IKzgF5GCqcSpjcXwzjswZQoceCD0nJI0axbs2QPXXx//2c0a6hGRghldVxt6MlM5T20spK4umDoVHnmktzZzJtx6K1jaJSvjPrtZwS8iBdPcND5jjB+q/zo9AO+9B/vtl1m7+GK4+27Yp49xlagHhItBwS8iBRP3nmypdXfDkKwUnTYNfvlLqKkJf085UPCLSEHFuSdbKu7he/J//SsccEDp+9lbOrgrIhKRezBWnx36HR3Ba5UQ+qDgFxGJJCzwt24NAv/gg+PpabAU/CIi/TDLnJEDsGFDEPiVerNABb+ISIiwwF+9Ogj8I46Ip6dCUfCLiKQ59NDcwF+xIgj8CRPi6anQFPwiIsCxxwaBv3Vrb23p0iDwP/Wp+PoqBgW/iCTaqacGgb96dW9t8eIg8E89Nb6+imnAefxmtgD4PLDV3Y9J1e4Dek7FqwM63H1iyHtfBd4CuoHdUe8HKSJSbOefD/fem1m75x646KJY2impKCdw3QPcDvysp+DuX+p5bGY/AN7s5/0nufvrg21QRKSQrrwSbr89s3bzzXD11fH0E4cBh3rc/QlgR9hrZmbANGBhgfsSESmoOXOCIZ300J8zJxjSSVLoQ/6XbPg/wF/cfX0frzvwmJk5cKe7z89zfSIie+WWW+DrX8+szZwJt90WTz/lIN/gP5/+9/Ynu3u7mY0ElprZy6nfIHKY2QxgBsDYsWPzbEtEku6ee+CSSzJr06fDQo1PDH5Wj5kNAc4F7utrGXdvT/25FVgMHN/PsvPdvdHdG+sr9XQ4EYnd4sXBkE566J9ySjCko9AP5DOd81TgZXffHPaimR1oZsN6HgOnA2vzWJ+ISJ9+85sg8M89t7c2YUIQ+L/5TXx9laMBg9/MFgJPA+PNbLOZXZp6aTpZwzxmNtrMeu49cyjwpJm9CDwLPOzuvy5c6yIiwVm1ZnDaab21kSODwE+fmy+9Bhzjd/fz+6hfHFJrB85KPd4IHJtnfyIiodasgU98IrfuXvpeKo1uxCIiFeWVV+CjH82tK/CjU/CLSEVob4eGkBt7KfD3noJfRMra9u0wYkRufc+e3KtoSjQKfhEpS2+9BR/4QG69uzv8frcSnYJfRMrKO+9AbW1uvasLhiixCkJ/jSJSFnbvhqFDc+vvvAP77Vf6fqqZfmESkVj1jNVnh/5bbwUHbhX6hafgF5FYuAeBX1OTWd++PXjtoIPi6SsJFPwiUnJmuQdo29uDwB8+PJ6ekkTBLyIlY5Y7BXPjxiDwR42Kp6ckUvCLSNGFBf7atUHgH354PD0lmYJfRIqmvj438J95Jgj8j388np5E0zlFpAjCzqh9/HE4+eTS9yK5tMcvIgVz8sm5ob94cbCHr9AvHwp+EcnbtGlB4C9f3lu77rog8KdOja8vCafgF5FBu+KKIPAfeKC3duutQeB/5zvx9SX9U/CLyF67/vog8P/933trN9wQBP6VV8bWlkSkg7siEtkPfwjXXJNZu+oquOWWePqRwVHwi8iAFiyASy/NrP3DP8B//mc8/Uh+otxsfYGZbTWztWm1G8yszcxeSH2d1cd7zzCzdWa2wcxmFbJxESm+Bx8MhnTSQ/+004IhHYV+5Yoyxn8PcEZI/WZ3n5j6eiT7RTOrAX4EnAkcDZxvZkfn06yIlMbSpUHgn3deb23ixCDwH3ssvr6kMAYc6nH3J8xs3CC+9/HABnffCGBm9wLnAH8cxPcSkRJ4+mn4zGcya6NGBRdQk+qRz6yemWa2OjUUdEjI6w3AprTnm1M1ESkzq1cHe/jpoW8W7OEr9KvPYIP/x8ARwERgC/CDkGXCboPsfX1DM5thZq1m1rpt27ZBtiUie2PDhiDgjz02s+4e3CBFqtOggt/d/+Lu3e6+B/gJwbBOts3AYWnPxwB97ju4+3x3b3T3xvr6+sG0JSIRtbUFgX/kkZl19+BLqtuggt/M0q+c/QVgbchizwFHmtnhZrYvMB14aDDrE5HCeP31IPDHjMms79mjwE+SAQ/umtlCYAowwsw2A3OAKWY2kWDo5lXgq6llRwN3uftZ7r7bzGYCLUANsMDd/1CUrRCRfm3fDiNG5NZ77ncryWJehv/NNzY2emtra9xtiFS8t9+GYcNy611dMESnb1YVM1vp7o1RltWPXqQKvfce7Ldfbn3XLqitLX0/1WLJqjbmtayjvaOT0XW1NDeNZ+qkypusqOAXqSLd3eF78m+8AXV1pe+nmixZ1cbsRWvo7OoGoK2jk9mL1gBUXPjr6pySGEtWtTF57jIOn/Uwk+cuY8mqtrhbKhj3YKw+O/S3bAleU+jnb17LuvdDv0dnVzfzWtbF1NHgaY9fEqGa9tayhR2cfeUV+MhHSt9LNWvv6NyrejnTHr8kQjXtrfUwyw39F18M9vAV+oU3ui784Ehf9XKm4JdEqKa9tbDAf/LJIPA/8Yl4ekqC5qbx1A6tyajVDq2huWl8TB0NnoZ6JBFG19XSFhLyUffWymE2xyGHQEdHZu3ZZ+G440raRmL1/Lzj/hwUgoJfEqG5aXzGGD9E31uL+/jAxz4GL7+cWVu2DE46qeirlixTJzVUZNBn01CPJMLUSQ3cdO4EGupqMaChrpabzp0Q6R9xXMcHPve5YEgnPfR/9atgSEehL/nQHr8kxmD31kp9fOC884I7X6X7+c/hgguKsjpJIO3xiwygVLM5Lrss2MNPD/3bbgv28BX6UkgKfpEBFHs2x7XXBoF/5529tW9/Owj8mTMLsgqRDBrqERlAsWZzfP/70NycWbv6arj55ry+rciAFPwiERRyNsddd8FXvpJZu/BC+NnPCvLtRQak4BcpkQcegGnTMmtnnAGPPhpPP5JcCn6RImtpCQI+3d/+LeiWExIXBb9IkTz1FHz2s5m1MWNg06Z4+hHpoeCXqlEOl1UAeOEFmDQpszZkSHDXK5FyoOCXqhD3ZRUA1q+Ho47KrZfh3U0l4Qacx29mC8xsq5mtTavNM7OXzWy1mS02s9DbPJjZq2a2xsxeMDONaErRxHnZ5c2bg3n42aHvrtCX8hTlBK57gKxDUywFjnH3TwB/Amb38/6T3H1i1JsAiwxGHJdd3rYtCPzDDsus79mjwJfyNmDwu/sTwI6s2mPuvjv1dAUwpgi9iURWyptk7NwZBP7IkZn1nsAPuyOWSDkpxCUb/hHoayayA4+Z2Uozm1GAdYmEKsVNMjo7g1A/+ODM+u7d8QR+Nd9DWIorr4O7ZnYdsBv4RR+LTHb3djMbCSw1s5dTv0GEfa8ZwAyAsWPH5tOWJFAxb5Lx7ruw//7h9X33zfvbD0o5HMyWymUeYTDSzMYB/+3ux6TVLgIuA05x910RvscNwNvu/v2Blm1sbPRWnd0iMevuDqZhZnv7bTjwwNL3k27y3GWhdxRrqKvlqVknx9CRxM3MVkY9ljqooR4zOwP4JnB2X6FvZgea2bCex8DpwNqwZUXKSc+wTXbo/+//Bq/FHfpQXfcQltKLMp1zIfA0MN7MNpvZpcDtwDCC4ZsXzOyO1LKjzeyR1FsPBZ40sxeBZ4GH3f3XRdkKkQIxg32y/lVs3BgE/qGHxtNTmFIezJbqM+AYv7ufH1K+u49l24GzUo83Asfm1Z1IiYQdmF2zBo45JrdeDvK5h7CIztwtsHK5bIBEExb4v/89nHBC6XvZG3t7MFufS0mn4C8gzbSoHGGB/+ijuVfRLGdR7xGgz6Vk060XCyjOywZINGa5ob9wYTCGX0mhvzf0uZRsCv4C0kyL8jV5cm7g/+hHQeBPnx5PT6Wiz6VkU/AXUNJnWpTjmaRf+EIQ+L//fW9t7twg8C+/PL6+Sinpn0vJpeAvoFJcNqBc9Ywjt3V04vSOI8cV/l/5ShD4S5b01nr28L/5zVhaik2SP5cSTsFfQFMnNXDTuRNoqKvFCM6ivOncCYk4gFYu48hXXBEE/l139dZuvDFZe/jZkvy5lHCa1VNgUWdaVJu4x5H//u+Dg7TprrkGvj/gBUKSIamfSwmn4JeCGF1XG3rtmGKPI3/963DLLZm1E0+E3/62qKsVqWga6pGCKPU48k03BUM66aFfUxMM6Sj0RfqnPX4piGJeFjnd7bfDlVfm1nXHK5HoFPwVqFxPvy/mOLICX6RwFPwVJmmn3997L5wfcplABb7I4GmMv8KUy7TJYlu6NBjDzw59d4W+SL60x19h4p42WWzPPQfHH59bV9iLFI72+CtMtZ5+v25dsIefHfrawxcpPAV/ham20+/b2oLA/5u/yazv2aPAFykWDfVUmFJNmyy2N96A4cNz6++9B0OHlr4fkSRR8FegSj79fteu8JuV//WvcMABpe9HJIkiDfWY2QIz22pma9Nqw81sqZmtT/15SB/vvSi1zHozu6hQjUtl6eoKhnSyQ3/79mBIR6EvUjpRx/jvAbLvTzQLeNzdjwQeTz3PYGbDgTnAp4DjgTl9/Qch1ck9CPx9982sb9oUvBY23CMixRUp+N39CWBHVvkc4Kepxz8Fpoa8tQlY6u473P0NYCm5/4FIlTKDfbI+YS+/HAT+mDHx9CQi+c3qOdTdtwCk/hwZskwDsCnt+eZULYeZzTCzVjNr3bZtWx5tSdzC7mv77LNB4I+vzMlHIlWl2NM5LaQWOknP3ee7e6O7N9bX1xe5LSmGsMB/7LEg8I87Lp6eRCRXPsH/FzMbBZD6c2vIMpuBw9KejwHa81inlKGwwF+4MAj8006LpycR6Vs+wf8Q0DNL5yLgVyHLtACnm9khqYO6p6dqUgXCAv/++4PAnz49np5EZGBRp3MuBJ4GxpvZZjO7FJgLnGZm64HTUs8xs0YzuwvA3XcA3waeS319K1WTChYW+HfcEQT+F78YT08iEp15GZ4X39jY6K2trXG3IVn23TeYj5/uxhvh2mvj6UdEepnZSndvjLKsrtUjAzriiGAPPz30r7oq2MNX6ItUHgW/9OnEE4PA37ixt/alLwWBn32DcxGpHLpWj+SYNg0eeCCzNmUKLF8eSzsiUmAKfnnfP/8z3HZbZu3II+FPf4qnHxEpDgW/8J3vwPXXZ9b23x86q+OmXiKSRcGfYD/+MVx+eW69DCd6iUgBKfgT6L77wk+wUuCLJIOCP0FaWuCMkGujKvBFkkXBnwArVsAJJ+TWFfgiyaTgr2LPPAOf/nRuXYEvkmwK/ir00ktw9NG5dQW+iICCvywsWdXGvJZ1tHd0Mrquluam8YO6mfqmTTB2bG59z57ci6qJSHIp+GO2ZFUbsxetobOrG4C2jk5mL1oDEDn8d+6Egw/OrXd1wRD9hEUki67VE7N5LeveD/0enV3dzGtZN+B733kn2JPPDv1du4JhHYW+iIRRNMSsvSP89Ni+6gC7d8PQobn1nTth2LBCdSYi1Up7/DEbXVcbue4e7OFnh/62bcFrCn0RiULBH7PmpvHUDq3JqNUOraG5aXxGzQz2yfppvfZaEPgjRhS7SxGpJhrqiVnPAdy+ZvWEzcZ55RX4yEdK2aWIVBMFfxmYOqkhZwZPWOCvXg0TJpSoKRGpWoMe6jGz8Wb2QtrXTjO7OmuZKWb2Ztoy/5p/y9Ut7EbmTz0VDOko9EWkEAa9x+/u64CJAGZWA7QBi0MW/Z27f36w60mKsD38X/8amppK30u5KtSJbiJJV6iDu6cAr7j7awX6fokxbFhu6N93X7CHr9Dv1XOiW1tHJ07viW5LVrXF3ZpIxSlU8E8HFvbx2glm9qKZPWpmH+/rG5jZDDNrNbPWbdu2Fait8nXUUUHgv/12b625OQj8adPi66tc5XOim4hkyjv4zWxf4GzggZCXnwc+7O7HArcBS/r6Pu4+390b3b2xvr4+37bK1pQpQeCvX99bu/HGIPC/973Y2ip7gznRTUTCFWKP/0zgeXf/S/YL7r7T3d9OPX4EGGpmiZx1/qUvBYH/29/21r7xjSDwr702vr4qxd6c6CYi/StE8J9PH8M8ZvYhs2AE28yOT61vewHWWTFmzgwC//77e2sXXhgE/g9+EF9flSbqiW4iMrC85vGb2QHAacBX02qXAbj7HcB5wNfMbDfQCUx3T8ZV4R94IHesvqkpmKkje2+gE91EJDorxxxubGz01tbWuNsYlPXrgwO36Q4Y9Ra/eHinQkpEisbMVrp7Y5RldeZugbz2Gowbl1k7cMImRpy1GoDZi4JhCoW/iMRNwZ+n9nb46EehM21yyVHnr+XdsZmnNPRMPVTwi0jcdHXOQdq2DerroaGhN/QXLAgO2r43Nvw8Nk09FJFyoODfS2+8AYcfDiNHwuuvB7Xbbw8C/5JLgueaeigi5UzBH9Fbb8Exx8Dw4fDqq0Hte98LAv+KKzKX1dRDESlnGuMfwK5dcNJJ8OyzvbU5c+CGG/p+TzlNPdSFzUQkm4K/D+++C2eeCcuX99aam+Hf/i38SprZwq6xX2o9FzbrucZNz4XNQLOLRJJMQz1Zurrg7LNh//17Q//yy2HPnmBoJ0rolwtd2ExEwmiPP6W7Gy64AO69t7f25S/Df/xH7r1uy01fwzm6sJmIhEl88O/ZAzNmwN1399bOPTe4Jv6QCvjb6W84Z3RdLW0hIa/ZRSLJVub7ssXjDlddBTU1vaHf1BSM7T/4YGWEPvQ/nKPZRSISpkLirXDc4V/+Bb773d7aZz8LS5cG4/qVpr/hnHKaXSQi5SNRwf/d78J11/U+nzgRfvc7OOig+HrK10DDOeUwu0hEyksihnpuuSWYjdMT+kcdBR0dsGpVZYc+6GQxEdl7Vb3H/5OfBAdue4weDatXwwc/GF9PhabhHBHZW1UZ/ItWtvF/G3uD74CD9vDK+n340IdibKqINJwjInuj6oJ/yao2Zv/XWmqGDaf7r/vR8NXlDPtgFyu2TGDqhxSOIiJVF/zzWtbxru9mzOXL3q91dqFr4YuIpFTdwV2drSoi0r+8g9/MXjWzNWb2gpnl3CjXArea2QYzW21mn8x3nf3RtfBFRPpXqD3+k9x9Yh83+j0TODL1NQP4cYHWGUrTG0VE+leKMf5zgJ+5uwMrzKzOzEa5+5ZirEzTG0VE+leI4HfgMTNz4E53n5/1egOwKe355lQtI/jNbAbBbwSMHTs2r4Y0vVFEpG+FGOqZ7O6fJBjSucLMTsx6PewK9p5TcJ/v7o3u3lhfX1+AtkREJEzewe/u7ak/twKLgeOzFtkMHJb2fAzQnu96RURkcPIKfjM70MyG9TwGTgfWZi32EPDl1OyeTwNvFmt8X0REBpbvGP+hwGIL7kc4BPilu//azC4DcPc7gEeAs4ANwC7gkjzXKSIiecgr+N19I3BsSP2OtMcOXJHPekREpHCq7sxdERHpn4JfRCRhFPwiIgmj4BcRSRgFv4hIwij4RUQSRsEvIpIwCn4RkYRR8IuIJIyCX0QkYRT8IiIJo+AXEUkYBb+ISMIo+EVEEkbBLyKSMAp+EZGEUfCLiCSMgl9EJGEGHfxmdpiZLTezl8zsD2Z2VcgyU8zsTTN7IfX1r/m1KyIi+crnnru7gWvc/XkzGwasNLOl7v7HrOV+5+6fz2M9IiJSQIPe43f3Le7+fOrxW8BLQEOhGhMRkeIoyBi/mY0DJgHPhLx8gpm9aGaPmtnHC7E+EREZvHyGegAws4OAB4Gr3X1n1svPAx9297fN7CxgCXBkH99nBjADYOzYsfm2VVGWrGpjXss62js6GV1XS3PTeKZO0i9PIlIcee3xm9lQgtD/hbsvyn7d3Xe6+9upx48AQ81sRNj3cvf57t7o7o319fX5tFVRlqxqY/aiNbR1dOJAW0cnsxetYcmqtrhbE5Eqlc+sHgPuBl5y9x/2scyHUsthZsen1rd9sOusRvNa1tHZ1Z1R6+zqZl7Lupg6EpFql89Qz2TgQmCNmb2Qql0LjAVw9zuA84CvmdluoBOY7u6exzqrTntH517VRUTyNejgd/cnARtgmduB2we7jiQYXVdLW0jIj66rjaEbEUkCnbkbs+am8dQOrcmo1Q6toblpfEwdiUi1y3tWj+SnZ/aOZvWISKko+MvA1EkNCnoRKRkN9YiIJIyCX0QkYRT8IiIJo+AXEUkYBb+ISMJYOZ5Ia2bbgNf6WWQE8HqJ2ik3Sd52SPb2a9uTKeq2f9jdI13orCyDfyBm1urujXH3EYckbzske/u17dr2QtFQj4hIwij4RUQSplKDf37cDcQoydsOyd5+bXsyFXzbK3KMX0REBq9S9/hFRGSQyjr4zewMM1tnZhvMbFbI6/uZ2X2p159J3fS9KkTY9m+Y2R/NbLWZPW5mH46jz2IYaNvTljvPzNzMqma2R5RtN7NpqZ/9H8zsl6XusVgifObHmtlyM1uV+tyfFUefxWBmC8xsq5mt7eN1M7NbU383q83sk3mt0N3L8guoAV4BPgLsC7wIHJ21zOXAHanH04H74u67hNt+EnBA6vHXkrTtqeWGAU8AK4DGuPsu4c/9SGAVcEjq+ci4+y7hts8HvpZ6fDTwatx9F3D7TwQ+Cazt4/WzgEcJbn71aeCZfNZXznv8xwMb3H2ju78H3Auck7XMOcBPU4//Czil5x6/FW7AbXf35e6+K/V0BTCmxD0WS5SfO8C3ge8B75SyuSKLsu1fAX7k7m8AuPvWEvdYLFG23YEPpB4fDLSXsL+icvcngB39LHIO8DMPrADqzGzUYNdXzsHfAGxKe745VQtdxt13A28CHyxJd8UVZdvTXUqwN1ANBtx2M5sEHObu/13Kxkogys/9KOAoM3vKzFaY2Rkl6664omz7DcAFZrYZeAS4sjStlYW9zYR+lfONWML23LOnIEVZphJF3i4zuwBoBD5X1I5Kp99tN7N9gJuBi0vVUAlF+bkPIRjumULwW97vzOwYd+8ocm/FFmXbzwfucfcfmNkJwM9T276n+O3FrqBZV857/JuBw9KejyH3V7v3lzGzIQS//vX361KliLLtmNmpwHXA2e7+bol6K7aBtn0YcAzw/8zsVYLxzoeq5ABv1M/8r9y9y93/B1hH8B9BpYuy7ZcC9wO4+9PA/gTXsUmCSJkQVTkH/3PAkWZ2uJntS3Dw9qGsZR4CLko9Pg9Y5qkjIRVuwG1PDXfcSRD61TLOCwNsu7u/6e4j3H2cu48jOL5xtru3xtNuQUX5zC8hOLCPmY0gGPrZWNIuiyPKtv8ZOAXAzD5GEPzbStplfB4Cvpya3fNp4E133zLYb1a2Qz3uvtvMZgItBEf8F7j7H8zsW0Cruz8E3E3w694Ggj396fF1XDgRt30ecBDwQOp49p/d/ezYmi6QiNtelSJuewtwupn9EegGmt19e3xdF0bEbb8G+ImZfZ1gmOPiKtnRw8wWEgzfjUgdw5gDDAVw9zsIjmmcBWwAdgGX5LW+Kvl7ExGRiMp5qEdERIpAwS8ikjAKfhGRhFHwi4gkjIJfRCRhFPwiIgmj4BcRSRgFv4hIwvx/8/NPOIoaUmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "X_data = np.random.random(20)\n",
    "Y_data = [13.4 * x + 5 + random.randint(-5, 5) for x in X_data]\n",
    "reg = LinearRegression().fit(X_data.reshape(-1,1), Y_data)\n",
    "reg.score(X_data.reshape(-1,1), Y_data)\n",
    "def f(x):\n",
    "    return reg.coef_*x + reg.intercept_\n",
    "print(\"The fit_fuction is: y = {}*x + {}\".format(float(reg.coef_), reg.intercept_))\n",
    "plt.scatter(X_data, Y_data)\n",
    "plt.plot(X_data, f(X_data), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09685196 0.16951955 0.21603456 0.25036642 0.25069536 0.27138126\n",
      " 0.29434957 0.30052351 0.31536473 0.31695336 0.3396437  0.35760148\n",
      " 0.3751618  0.37581354 0.44590435 0.46812584 0.48032066 0.51058315\n",
      " 0.57998033 0.59765494 0.60187178 0.60702034 0.6109326  0.62948463\n",
      " 0.63107921 0.63372967 0.69593493 0.73018951 0.78081718 0.79298167\n",
      " 0.79665617 0.81237612 0.84831297 0.8533915  0.85515944 0.87371364\n",
      " 0.880877   0.90291631 0.94384388 0.9773082 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "X_data = np.random.random(40)\n",
    "X_data.sort()\n",
    "y = np.array([13.4 * x + 5 + random.randint(-5, 5) for x in X_data])\n",
    "\n",
    "#X_data_test = np.random.random(10)\n",
    "#Y_data_test = np.array([13.4 * x + 5 + random.randint(-5, 5) for x in X_data_test])\n",
    "\n",
    "def model(X, y):\n",
    "    return [(X[i], y[i]) for i in range(len(X))]\n",
    "\n",
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)\n",
    "\n",
    "def predict(xi, k):\n",
    "    distances = [distance(xi, x) for x in X_data]\n",
    "    k_neighbors = np.argsort(distances)[:k]\n",
    "    #predict_y  = np.mean(y[k_neighbors])  ##以最近k个点的平均值作为预测点的y值\n",
    "    count = Counter(y[k_neighbors])\n",
    "    predict_y = count.most_common()[0][0]    ##距离最近点作为预测点的y值\n",
    "    return predict_y\n",
    "\n",
    "predict_y_list_test = []\n",
    "for x in X_data:\n",
    "    predict_y_list_test.append(predict(x,5))\n",
    "#print(predict_y_list_test)\n",
    "plt.scatter(X_data, y, color = 'red')\n",
    "plt.plot(X_data, predict_y_list_test, color='black', lw=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| variable: 'income'\n",
      "ic| values: {'-10', '+10'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "1      M    -10              1       1\n",
      "2      F    +10              2       1\n",
      "3      F    +10              1       0\n",
      "4      M    +10              2       0\n",
      "5      F    +10              1       0\n",
      "6      M    -10              2       1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_sub_spliter_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_sub_spliter_2: 0.6730116670092565\n",
      "ic| value_entropy: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_sub_spliter_2: -0.0\n",
      "ic| value_entropy: 0.6730116670092565\n",
      "ic| variable: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 0, 1]\n",
      "ic| entropy_sub_spliter_2: 0.6365141682948128\n",
      "ic| value_entropy: 1.3296613488547582\n",
      "ic| sub_spliter_1: [1, 0, 1]\n",
      "ic| entropy_sub_spliter_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 0, 0]\n",
      "ic| entropy_sub_spliter_2: 0.6931471805599453\n",
      "ic| value_entropy: 1.3296613488547582\n",
      "ic| variable: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [1, 0, 1]\n",
      "ic| entropy_sub_spliter_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 0, 0]\n",
      "ic| entropy_sub_spliter_2: 0.6931471805599453\n",
      "ic| value_entropy: 1.3296613488547582\n",
      "ic| sub_spliter_1: [1, 1, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 0, 1]\n",
      "ic| entropy_sub_spliter_2: 0.6365141682948128\n",
      "ic| value_entropy: 1.3296613488547582\n",
      "ic| variable: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_sub_spliter_1:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spliter is ('income', '+10')\n",
      "The min_entrop is 0.6730116670092565\n",
      "  gender income  family_number  bought\n",
      "0      F    +10              1       1\n",
      "1      M    -10              1       1\n",
      "3      F    +10              1       0\n",
      "5      F    +10              1       0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0]\n",
      "ic| entropy_sub_spliter_2: 0.6365141682948128\n",
      "ic| value_entropy: 0.6365141682948128\n",
      "ic| sub_spliter_1: [1, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_sub_spliter_2: -0.0\n",
      "ic| value_entropy: 0.6365141682948128\n",
      "ic| variable: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_sub_spliter_2: 0\n",
      "ic| value_entropy: 0.6931471805599453\n",
      "ic| variable: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_sub_spliter_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0]\n",
      "ic| entropy_sub_spliter_2: 0.6365141682948128\n",
      "ic| value_entropy: 0.6365141682948128\n",
      "ic| sub_spliter_1: [1, 0, 0]\n",
      "ic| entropy_sub_spliter_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_sub_spliter_2: -0.0\n",
      "ic| value_entropy: 0.6365141682948128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spliter is ('gender', 'F')\n",
      "The min_entrop is 0.6365141682948128\n",
      "  gender income  family_number  bought\n",
      "1      M    -10              1       1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "def entropy(element): \n",
    "    count = Counter(element)\n",
    "    possibility = [count[a] / len(element) for a in set(element)]\n",
    "    entropy_1 = -sum(b * np.log(b) for b in possibility)\n",
    "    return entropy_1\n",
    "\n",
    "#print(entropy([1, 1, 1, 0]))\n",
    "    \n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "print(dataset)\n",
    "\n",
    "def find_min_spliter(training_data, target:str):\n",
    "    x_params = set(training_data.columns.tolist()) - {target}\n",
    "    min_spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    for variable in x_params:\n",
    "        ic(variable)\n",
    "        values = set(training_data[variable])\n",
    "        ic(values)\n",
    "        for value in values:\n",
    "            sub_spliter_1 = training_data[training_data[variable]==value][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            entropy_sub_spliter_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_sub_spliter_1)\n",
    "            sub_spliter_2 = training_data[training_data[variable]!=value][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            entropy_sub_spliter_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_sub_spliter_2)\n",
    "            value_entropy = entropy_sub_spliter_1 + entropy_sub_spliter_2\n",
    "            ic(value_entropy)\n",
    "            \n",
    "            if value_entropy <= min_entropy:\n",
    "                min_entropy = value_entropy\n",
    "                min_spliter = (variable, value)\n",
    "    print('The spliter is {}'.format(min_spliter))\n",
    "    print('The min_entrop is {}'.format(min_entropy))\n",
    "    return min_spliter\n",
    "\n",
    "find_min_spliter(dataset, target='bought')\n",
    "print(dataset[dataset['family_number']!=2])\n",
    "find_min_spliter(dataset[dataset['family_number']!=2],target='bought')\n",
    "\n",
    "dataset_spliter1 = dataset[dataset['family_number']!=2]\n",
    "print(dataset_spliter1[dataset_spliter1['income']!='+10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {7: [[43, 63], [48, 62], [55, 50], [59, 58], [41, 35], [53, 55], [39, 46], [44, 46], [52, 56], [59, 40], [55, 62], [64, 58], [37, 60], [43, 61], [53, 62], [59, 41], [34, 57]], 2: [[76, 98], [56, 77], [82, 74], [61, 77], [55, 85], [67, 97], [97, 95], [64, 85], [52, 93], [69, 90], [54, 83], [73, 91], [82, 76], [98, 90]], 6: [[81, 22], [86, 31], [67, 30], [84, 18], [65, 18], [66, 19], [70, 10], [65, 28], [61, 15], [68, 16], [95, 0], [81, 26], [80, 27], [82, 28], [79, 17], [81, 29], [66, 7], [80, 20], [93, 15]], 5: [[22, 82], [13, 93], [28, 98], [32, 78], [16, 76], [33, 76], [1, 82], [34, 92], [4, 71], [40, 76], [43, 95], [29, 83], [15, 75], [0, 88], [19, 64]], 3: [[0, 2], [4, 6], [1, 10], [26, 5], [24, 12], [1, 5], [13, 1], [14, 14]], 4: [[36, 17], [51, 11], [55, 24], [44, 0], [32, 13], [35, 6], [54, 3], [36, 3], [49, 14], [43, 23], [38, 19]], 0: [[16, 46], [16, 29], [4, 38], [6, 30], [27, 55], [2, 36], [27, 47], [33, 39]], 1: [[100, 42], [98, 42], [80, 53], [97, 32], [93, 56], [96, 53], [92, 62], [90, 55]]})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [random.randint(0, 100) for _ in range(100)]\n",
    "Y = [random.randint(0, 100) for _ in range(100)]\n",
    "\n",
    "training_data = [[x, y] for x, y in zip(X, Y)]\n",
    "cluster = KMeans(n_clusters=8, max_iter=600)\n",
    "cluster.fit(training_data)\n",
    "cluster.cluster_centers_\n",
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, training_data):\n",
    "    centers[label].append(location)\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue', 'grey', 'black', 'purple']\n",
    "#print(centers)\n",
    "for index, value in enumerate(centers):\n",
    "    for location in centers[value]:\n",
    "        plt.scatter(*location, c=colors[index])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What's the model? why all the models are wrong, but some are useful? (5 points)¶\n",
    "\n",
    "模型是用来利用已知数据来预测未来数据（或是建立数据预测和分类的方法，或根据数据内部之间的联系来建立分类和预测方法）的数学工具（function）。在建立模型过程中，为考虑其数学可实现性、简易性以及其他一些方面的问题，会对模型进行某种程度的简化（相对于复杂的实际问题），因此模型并不能完全代表实际问题（all the models are wrong），但是模型的简化是基于忽略一些对实际问题影响很微小的变量的原则，因此其准确性很高（例如朴素贝叶斯垃圾邮件分类方法会忽略x2，x3，x4...对x1是否垃圾邮件的影响，只考虑spam对其的影响），完全可以应用于工程实际问题（but some are useful）。\n",
    "\n",
    "\n",
    "2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)\n",
    "\n",
    "欠拟合是指模型在训练集和测试集上表现的都不好（模型过于简单的原因），模型学习能力不足，因此模型误差较大（Bias大，Variance大），很难反应数据的真实分布情况；过拟合是指模型在训练集上效果很好，但是泛化性能较差（Variance大）,在测试集上表现很差。模型过于简单，模型的特征量太少会导致欠拟合；参数太多（模型复杂度高），样本噪音数据干扰大，特征选取错误，学习迭代次数过多会导致过拟合。\n",
    "\n",
    "\n",
    "3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')\n",
    "\n",
    "precision针对于模型的预测结果而言，指的是：对于二分类问题，例如针对某测试集，有AB两类，模型预测了100个A类（B类），其中98个是对的A类（B类），则模型的precision为98%；\n",
    "\n",
    "recall针对于测试样本，例如:测试数据集中总共有100个A类（B类），模型能够成功准确预测的A类（B类）有90个，则recall为90%，其一个特点是不受样本不均衡的影响（例如垃圾邮件分类中，500封邮件只有2封垃圾邮件，498封好的邮件，则precision会受到样本不均衡的影响，recall则不会。）\n",
    "\n",
    "F1为precision和recall的调和平均数，F1=2*precision*recall/(precision+recall),用于兼顾模型的precision和recall（以更全面评价模型的优劣），且可通过系数来调整两者的权重(Fα=(α^2+1)*precision/(α^2*precision+recall)),F2的意思是recall的权重高于precision；\n",
    "\n",
    "AUC用来评价模型性能优劣以及找到最佳指标值的阈值，例如：在a个A类样本中，分类模型准确分类出a1个A类样本，在b个B类样本，分类模型准确分类出b1个B类样本，则分类模型的AUC=(a1/a)*(b1/b)。同样，recall不受样本不均衡的影响，其值越接近1，则分类模型性能越好。\n",
    "\n",
    "\n",
    "4. Based on our course and yourself mind, what's the machine learning? (8')\n",
    "\n",
    "机器学习是针对一些已经获得的学习样本数据和测试数据，来训练学习得到某个模型（即某个逻辑加工过程），该模型能够准确预测未来数据，或者对数据进行分类并能够准确找到未知数据的类别。机器学习过程可分为有监督和无监督学习，无监督学习是指不需要给数据任何标签，而只是根据数据内部之间的关系，对其进行分类和分类预测（KNN，聚类等问题），有监督学习则是指：我们提前知道一些输入数据集，并知道他们应该的输出，机器学习的目的就是找到输入输出的关系（线性回归，决策树等问题）。\n",
    "\n",
    "5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)\n",
    "\n",
    "这句话突出了评价标准对于机器学习模型的重要性。机器学习模型的评价标准有很多种，包括：误差、准确率、精确率、召回率、F值、ROC-AUC、PRC-AUC等。因此要评价模型的性能优劣，需要根据具体的训练集数据特点和预测模型要求等来选择，例如：我们最常用也最直观的指标是准确率，但是准确率在评价某些模型时会存在较大“欺骗性”（特别是样本数据很不均衡时），邮件分类问题，如果预测样本数据集中有500封邮件，其中495封垃圾邮件，5封正常邮件，那么即使模型将全部5封正常邮件分类为垃圾邮件，模型的垃圾邮件预测准确率也很高，如果我们更关注5封正常邮件的分类，则会带来很大困扰，因此该模型还需要recall，Fα或者AUC等指标来综合评价。综上，评价标准对于正确评价机器学习模型的重要性不言而喻，需要根据数据集特点和其他条件，综合评价指标，得到更好的结果。\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a completed Decision Tree Model. You show finish a predicate() function, which accepts three parameters <gender, income, family_number>, and outputs the predicated 'bought': 1 or 0. (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'family_number': {1: {'income': {'+10': {'gender': {'M': 0, 'F': 1}}, '-10': 1}}, 2: 1}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from icecream import ic\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "\n",
    "def entropy(element): \n",
    "    count = Counter(element)\n",
    "    possibility = [count[a] / len(element) for a in set(element)]\n",
    "    entropy_1 = -sum(b * np.log(b) for b in possibility)\n",
    "    return entropy_1\n",
    "\n",
    "#print(entropy([1, 1, 1, 0]))\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "variables = dataset.columns.tolist()\n",
    "\n",
    "def find_min_spliter(training_data, target:str):\n",
    "    x_params = set(training_data.columns.tolist()) - {target}\n",
    "    min_spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    for variable in x_params:\n",
    "        #ic(variable)\n",
    "        values = set(training_data[variable])\n",
    "        #ic(values)\n",
    "        for value in values:\n",
    "            sub_spliter_1 = training_data[training_data[variable]==value][target].tolist()\n",
    "            #ic(sub_spliter_1)\n",
    "            entropy_sub_spliter_1 = entropy(sub_spliter_1)\n",
    "            #ic(entropy_sub_spliter_1)\n",
    "            sub_spliter_2 = training_data[training_data[variable]!=value][target].tolist()\n",
    "            #ic(sub_spliter_2)\n",
    "            entropy_sub_spliter_2 = entropy(sub_spliter_2)\n",
    "            #ic(entropy_sub_spliter_2)\n",
    "            value_entropy = entropy_sub_spliter_1 + entropy_sub_spliter_2\n",
    "            #ic(value_entropy)\n",
    "            \n",
    "            if value_entropy <= min_entropy:\n",
    "                min_entropy = value_entropy\n",
    "                min_spliter = (variable, value)\n",
    "    #print('The spliter is {}'.format(min_spliter))\n",
    "    #print('The min_entrop is {}'.format(min_entropy))\n",
    "    return min_spliter\n",
    "\n",
    "def majority_label(label_list):\n",
    "    count = Counter(label_list)\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "\n",
    "def Decision_tree(dataset):\n",
    "    labels = [dataset.values[i][-1] for i in range(dataset.shape[0])]\n",
    "    variables = dataset.columns.tolist()\n",
    "    target = variables[-1]\n",
    "    if len(variables) == 1:\n",
    "        return labels[0]\n",
    "    if len(set(labels)) == 1:  \n",
    "        return dataset.values[0][-1]\n",
    "    min_spliter = find_min_spliter(dataset, target)\n",
    "    min_spliter_variable = min_spliter[0]\n",
    "    min_spliter_value = min_spliter[1]\n",
    "    min_spliter_values = dataset[min_spliter_variable].values.tolist()\n",
    "    label_list = []\n",
    "    for i in range(len(min_spliter_values)):\n",
    "        if min_spliter_values[i] == min_spliter_value:\n",
    "            label_list.append(dataset[target].values.tolist()[i])\n",
    "    decisi_tree = {min_spliter_variable:{}}\n",
    "    dataset_next = dataset[dataset[min_spliter_variable]!=min_spliter_value].drop([min_spliter_variable], axis=1)\n",
    "    for value in set(min_spliter_values):\n",
    "        if value == min_spliter_value:\n",
    "            decisi_tree[min_spliter_variable][value] = majority_label(label_list)\n",
    "        else:\n",
    "            decisi_tree[min_spliter_variable][value] = Decision_tree(dataset_next)\n",
    "    return decisi_tree\n",
    "\n",
    "decision_tree = Decision_tree(dataset)\n",
    "print(decision_tree)\n",
    "#find_min_spliter(dataset, target='bought')\n",
    "#print(dataset[dataset['income']=='-10'])\n",
    "#find_min_spliter(dataset[dataset['income']=='-10'],target='bought')\n",
    "\n",
    "#dataset_spliter1 = dataset[dataset['income']=='-10']\n",
    "#print(dataset_spliter1[dataset_spliter1['family_number']==1])\n",
    "#dataset_spliter1[dataset_spliter1['gender'] == 'F']\n",
    "#find_min_spliter(dataset_spliter1[dataset_spliter1['income']=='-10'],target='bought')\n",
    "\n",
    "def predicate(deci_tree, test_variables):\n",
    "    first_spliter_variable = list(deci_tree.keys())[0]\n",
    "    dicts = deci_tree[first_spliter_variable]\n",
    "    x_variables = dataset.columns.tolist()[:-1]\n",
    "    variable_index = x_variables.index(first_spliter_variable)\n",
    "    for key in dicts.keys():\n",
    "        if test_variables[variable_index] == key:\n",
    "            if type(dicts[key]).__name__ == 'dict':\n",
    "                predicate_label = predicate(dicts[key], test_variables)\n",
    "            else:\n",
    "                predicate_label = dicts[key]\n",
    "                \n",
    "    return predicate_label\n",
    "\n",
    "predicate(decision_tree, ['M', '-10', 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 251428.27931664474, parameters k is 81.66093258968979 and b is 6.217533909320693\n",
      "Iteration 1, the loss is 17095.61794690761, parameters k is 18.06043258968971 and b is -57.38296609067939\n",
      "Iteration 2, the loss is 214744.18707935585, parameters k is -44.80930741031038 and b is -120.25270609067948\n",
      "Iteration 3, the loss is 19764.163087775054, parameters k is 18.7911925896897 and b is -56.6522060906794\n",
      "Iteration 4, the loss is 212916.80332535607, parameters k is -44.31354741031039 and b is -119.75694609067949\n",
      "Iteration 5, the loss is 21576.283382895002, parameters k is 19.28695258968969 and b is -56.15644609067941\n",
      "Iteration 6, the loss is 211089.41957135624, parameters k is -43.8177874103104 and b is -119.2611860906795\n",
      "Iteration 7, the loss is 23388.403678014998, parameters k is 19.78271258968968 and b is -55.66068609067942\n",
      "Iteration 8, the loss is 209262.03581735634, parameters k is -43.32202741031041 and b is -118.76542609067951\n",
      "Iteration 9, the loss is 25200.66560018152, parameters k is 20.27847258968967 and b is -55.16492609067943\n",
      "Iteration 10, the loss is 208004.2166463563, parameters k is -42.98078741031042 and b is -118.42418609067951\n",
      "Iteration 11, the loss is 26451.297574301476, parameters k is 20.619712589689662 and b is -54.82368609067943\n",
      "Iteration 12, the loss is 206746.39747535624, parameters k is -42.63954741031043 and b is -118.08294609067951\n",
      "Iteration 13, the loss is 27701.929548421485, parameters k is 20.960952589689654 and b is -54.48244609067943\n",
      "Iteration 14, the loss is 205488.57830435628, parameters k is -42.298307410310436 and b is -117.74170609067951\n",
      "Iteration 15, the loss is 28956.022904701593, parameters k is 21.302192589689646 and b is -54.14120609067943\n",
      "Iteration 16, the loss is 204963.54090335642, parameters k is -42.155867410310435 and b is -117.59926609067952\n",
      "Iteration 17, the loss is 29479.760968021616, parameters k is 21.444632589689647 and b is -53.99876609067944\n",
      "Iteration 18, the loss is 204438.50350235633, parameters k is -42.013427410310435 and b is -117.45682609067953\n",
      "Iteration 19, the loss is 30003.499031341606, parameters k is 21.587072589689647 and b is -53.856326090679445\n",
      "Iteration 20, the loss is 203913.46610135626, parameters k is -41.870987410310434 and b is -117.31438609067953\n",
      "Iteration 21, the loss is 30527.23709466162, parameters k is 21.729512589689648 and b is -53.71388609067945\n",
      "Iteration 22, the loss is 203388.42870035642, parameters k is -41.72854741031043 and b is -117.17194609067954\n",
      "Iteration 23, the loss is 31050.975157981607, parameters k is 21.87195258968965 and b is -53.57144609067946\n",
      "Iteration 24, the loss is 202863.39129935636, parameters k is -41.58610741031043 and b is -117.02950609067955\n",
      "Iteration 25, the loss is 31574.713221301634, parameters k is 22.01439258968965 and b is -53.429006090679465\n",
      "Iteration 26, the loss is 202338.3538983559, parameters k is -41.44366741031043 and b is -116.88706609067955\n",
      "Iteration 27, the loss is 32098.4512846216, parameters k is 22.15683258968965 and b is -53.28656609067947\n",
      "Iteration 28, the loss is 201813.31649735617, parameters k is -41.30122741031043 and b is -116.74462609067956\n",
      "Iteration 29, the loss is 32622.189347941603, parameters k is 22.29927258968965 and b is -53.14412609067948\n",
      "Iteration 30, the loss is 201288.27909635636, parameters k is -41.15878741031043 and b is -116.60218609067957\n",
      "Iteration 31, the loss is 33145.9274112616, parameters k is 22.44171258968965 and b is -53.001686090679485\n",
      "Iteration 32, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 33, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 34, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 35, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 36, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 37, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 38, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 39, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 40, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 41, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 42, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 43, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 44, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 45, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 46, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 47, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 48, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 49, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 50, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 51, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 52, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 53, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 54, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 55, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 56, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 57, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 58, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 59, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 60, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 61, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 62, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 63, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 64, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 65, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 66, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 67, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 68, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 69, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 70, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 71, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 72, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 73, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 74, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 75, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 76, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 77, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 78, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 79, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 80, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 81, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 82, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 84, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 85, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 86, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 87, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 88, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 89, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 90, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 91, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 92, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 93, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 94, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 95, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 96, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 97, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 98, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 99, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 100, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 101, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 102, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 103, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 104, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 105, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 106, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 107, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 108, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 109, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 110, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 111, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 112, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 113, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 114, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 115, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 116, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 117, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 118, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 119, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 120, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 121, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 122, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 123, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 124, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 125, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 126, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 127, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 128, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 129, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 130, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 131, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 132, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 133, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 134, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 135, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 136, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 137, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 138, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 139, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 140, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 141, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 142, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 143, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 144, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 145, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 146, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 147, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 148, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 149, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 150, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 151, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 152, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 153, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 154, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 155, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 156, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 157, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 158, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 159, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 160, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 161, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 162, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 163, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 164, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 165, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 166, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 167, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 168, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 169, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 170, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 171, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 172, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 173, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 174, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 175, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 176, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 177, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 178, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 179, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 180, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 181, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 182, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 183, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 184, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 185, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 186, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 187, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 188, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 189, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 190, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 191, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 192, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 193, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 194, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 195, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 196, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 197, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 198, the loss is 200763.24169535644, parameters k is -41.01634741031043 and b is -116.45974609067957\n",
      "Iteration 199, the loss is 33669.79131714403, parameters k is 22.58415258968965 and b is -52.85924609067949\n",
      "Iteration 0, the loss is 16556.801294775178, parameters k is -5.291949482432273 and b is 88.27219415225323\n",
      "Iteration 1, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 2, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 3, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 4, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 5, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 6, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 7, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 8, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 9, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 10, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 11, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 12, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 13, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 14, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 15, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 16, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 17, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 18, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 19, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 20, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 21, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 22, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 23, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 24, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 25, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 26, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 27, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 28, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 29, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 30, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 31, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 32, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 33, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 34, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 35, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 36, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 37, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 38, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 39, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 40, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 41, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 42, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 43, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 44, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 45, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 46, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 47, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 48, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 49, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 50, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 51, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 52, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 53, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 54, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 55, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 56, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 57, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 58, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 59, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 60, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 61, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 62, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 63, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 64, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 65, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 66, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 67, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 68, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 69, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 70, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 71, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 72, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 73, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 74, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 75, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 76, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 77, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 78, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 79, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 80, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 81, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 82, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 83, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 84, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 85, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 86, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 87, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 88, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 89, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 90, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 91, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 92, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 93, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 94, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 95, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 96, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 97, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 98, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 99, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 100, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 101, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 102, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 103, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 104, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 105, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 106, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 107, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 108, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 109, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 110, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 111, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 112, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 113, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 114, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 115, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 116, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 117, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 118, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 119, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 120, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 121, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 122, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 123, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 124, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 125, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 126, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 127, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 128, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 129, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 130, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 131, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 132, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 133, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 134, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 135, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 136, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 137, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 138, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 139, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 140, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 141, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 142, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 143, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 144, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 145, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 146, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 147, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 148, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 149, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 150, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 151, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 152, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 153, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 154, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 155, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 156, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 157, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 158, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 159, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 160, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 161, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 162, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 163, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 164, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 165, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 166, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 167, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 168, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 169, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 170, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 171, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 172, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 173, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 174, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 175, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 176, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 177, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 178, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 179, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 180, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 181, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 182, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 183, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 184, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 185, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 186, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 187, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 188, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 189, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 190, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 191, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 192, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 193, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 194, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 195, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 196, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 197, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 198, the loss is 25410.92202216846, parameters k is -2.856989482432269 and b is 90.70715415225324\n",
      "Iteration 199, the loss is 91805.59448408174, parameters k is -34.65723948243231 and b is 58.9069041522532\n",
      "Iteration 0, the loss is 172818.90395139236, parameters k is -60.06937103147756 and b is 58.5075052628103\n",
      "Iteration 1, the loss is 149375.60065014235, parameters k is -53.70932103147755 and b is 64.8675552628103\n",
      "Iteration 2, the loss is 125932.29734889239, parameters k is -47.34927103147754 and b is 71.2276052628103\n",
      "Iteration 3, the loss is 102488.99404764237, parameters k is -40.989221031477534 and b is 77.5876552628103\n",
      "Iteration 4, the loss is 79045.69074639225, parameters k is -34.629171031477526 and b is 83.9477052628103\n",
      "Iteration 5, the loss is 55602.38744514225, parameters k is -28.269121031477518 and b is 90.3077552628103\n",
      "Iteration 6, the loss is 32159.084143892254, parameters k is -21.90907103147751 and b is 96.6678052628103\n",
      "Iteration 7, the loss is 9728.168655993919, parameters k is -15.549021031477501 and b is 103.0278552628103\n",
      "Iteration 8, the loss is 11717.784146489606, parameters k is -10.367403031477497 and b is 108.2094732628103\n",
      "Iteration 9, the loss is 8439.511495562561, parameters k is -15.0887370314775 and b is 103.48813926281031\n",
      "Iteration 10, the loss is 11232.847528883774, parameters k is -10.5466070314775 and b is 108.03026926281031\n",
      "Iteration 11, the loss is 8384.961164542576, parameters k is -15.067829031477501 and b is 103.5090472628103\n",
      "Iteration 12, the loss is 11288.241685018205, parameters k is -10.525699031477501 and b is 108.0511772628103\n",
      "Iteration 13, the loss is 8404.873548622565, parameters k is -15.075461031477504 and b is 103.5014152628103\n",
      "Iteration 14, the loss is 11267.957384519768, parameters k is -10.533331031477505 and b is 108.0435452628103\n",
      "Iteration 15, the loss is 8350.4599964937, parameters k is -15.054553031477507 and b is 103.52232326281029\n",
      "Iteration 16, the loss is 11263.345182935751, parameters k is -10.535075031477508 and b is 108.04180126281028\n",
      "Iteration 17, the loss is 8354.9869653097, parameters k is -15.05629703147751 and b is 103.52057926281029\n",
      "Iteration 18, the loss is 11258.732981351734, parameters k is -10.53681903147751 and b is 108.04005726281028\n",
      "Iteration 19, the loss is 8359.513934125709, parameters k is -15.058041031477511 and b is 103.51883526281028\n",
      "Iteration 20, the loss is 11254.120779767725, parameters k is -10.538563031477512 and b is 108.03831326281028\n",
      "Iteration 21, the loss is 8364.040902941706, parameters k is -15.059785031477514 and b is 103.51709126281028\n",
      "Iteration 22, the loss is 11249.508578183726, parameters k is -10.540307031477514 and b is 108.03656926281027\n",
      "Iteration 23, the loss is 8368.567871757718, parameters k is -15.061529031477516 and b is 103.51534726281028\n",
      "Iteration 24, the loss is 11244.896376599718, parameters k is -10.542051031477516 and b is 108.03482526281027\n",
      "Iteration 25, the loss is 8373.094840573722, parameters k is -15.063273031477518 and b is 103.51360326281028\n",
      "Iteration 26, the loss is 11240.284175015726, parameters k is -10.543795031477519 and b is 108.03308126281027\n",
      "Iteration 27, the loss is 8377.624473762611, parameters k is -15.06501703147752 and b is 103.51185926281028\n",
      "Iteration 28, the loss is 11295.724082390157, parameters k is -10.52288703147752 and b is 108.05398926281028\n",
      "Iteration 29, the loss is 8397.53685784264, parameters k is -15.072649031477523 and b is 103.50422726281028\n",
      "Iteration 30, the loss is 11275.416238598142, parameters k is -10.530519031477525 and b is 108.04635726281028\n",
      "Iteration 31, the loss is 8417.449241922639, parameters k is -15.080281031477528 and b is 103.49659526281027\n",
      "Iteration 32, the loss is 11255.210359499695, parameters k is -10.538151031477529 and b is 108.03872526281027\n",
      "Iteration 33, the loss is 8362.971458473747, parameters k is -15.05937303147753 and b is 103.51750326281027\n",
      "Iteration 34, the loss is 11250.598157915683, parameters k is -10.539895031477531 and b is 108.03698126281026\n",
      "Iteration 35, the loss is 8367.498427289762, parameters k is -15.061117031477533 and b is 103.51575926281026\n",
      "Iteration 36, the loss is 11245.985956331679, parameters k is -10.541639031477533 and b is 108.03523726281026\n",
      "Iteration 37, the loss is 8372.025396105759, parameters k is -15.062861031477535 and b is 103.51401526281026\n",
      "Iteration 38, the loss is 11241.373754747685, parameters k is -10.543383031477536 and b is 108.03349326281025\n",
      "Iteration 39, the loss is 8376.552364921776, parameters k is -15.064605031477537 and b is 103.51227126281026\n",
      "Iteration 40, the loss is 11236.761553163677, parameters k is -10.545127031477538 and b is 108.03174926281025\n",
      "Iteration 41, the loss is 8381.099748342667, parameters k is -15.06634903147754 and b is 103.51052726281026\n",
      "Iteration 42, the loss is 11292.1797888981, parameters k is -10.52421903147754 and b is 108.05265726281026\n",
      "Iteration 43, the loss is 8401.012132422678, parameters k is -15.073981031477542 and b is 103.50289526281026\n",
      "Iteration 44, the loss is 11271.871945106093, parameters k is -10.531851031477544 and b is 108.04502526281026\n",
      "Iteration 45, the loss is 8420.924516502693, parameters k is -15.081613031477547 and b is 103.49526326281025\n",
      "Iteration 46, the loss is 11251.687737647646, parameters k is -10.539483031477548 and b is 108.03739326281026\n",
      "Iteration 47, the loss is 8366.4289828218, parameters k is -15.06070503147755 and b is 103.51617126281025\n",
      "Iteration 48, the loss is 11247.07553606363, parameters k is -10.54122703147755 and b is 108.03564926281024\n",
      "Iteration 49, the loss is 8370.955951637814, parameters k is -15.062449031477552 and b is 103.51442726281024\n",
      "Iteration 50, the loss is 11242.463334479613, parameters k is -10.542971031477553 and b is 108.03390526281024\n",
      "Iteration 51, the loss is 8375.482920453815, parameters k is -15.064193031477554 and b is 103.51268326281024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, the loss is 11237.85113289561, parameters k is -10.544715031477555 and b is 108.03216126281023\n",
      "Iteration 53, the loss is 8380.024813562717, parameters k is -15.065937031477556 and b is 103.51093926281024\n",
      "Iteration 54, the loss is 11293.276071870054, parameters k is -10.523807031477556 and b is 108.05306926281024\n",
      "Iteration 55, the loss is 8399.937197642725, parameters k is -15.07356903147756 and b is 103.50330726281024\n",
      "Iteration 56, the loss is 11272.96822807805, parameters k is -10.53143903147756 and b is 108.04543726281024\n",
      "Iteration 57, the loss is 8419.849581722741, parameters k is -15.081201031477564 and b is 103.49567526281024\n",
      "Iteration 58, the loss is 11252.777317379585, parameters k is -10.539071031477565 and b is 108.03780526281024\n",
      "Iteration 59, the loss is 8365.359538353841, parameters k is -15.060293031477567 and b is 103.51658326281023\n",
      "Iteration 60, the loss is 11248.165115795593, parameters k is -10.540815031477567 and b is 108.03606126281022\n",
      "Iteration 61, the loss is 8369.88650716985, parameters k is -15.06203703147757 and b is 103.51483926281023\n",
      "Iteration 62, the loss is 11243.552914211588, parameters k is -10.54255903147757 and b is 108.03431726281022\n",
      "Iteration 63, the loss is 8374.413475985863, parameters k is -15.063781031477571 and b is 103.51309526281022\n",
      "Iteration 64, the loss is 11238.940712627587, parameters k is -10.544303031477572 and b is 108.03257326281022\n",
      "Iteration 65, the loss is 8378.949878782756, parameters k is -15.065525031477573 and b is 103.51135126281022\n",
      "Iteration 66, the loss is 11294.372354842022, parameters k is -10.523395031477573 and b is 108.05348126281022\n",
      "Iteration 67, the loss is 8398.862262862762, parameters k is -15.073157031477576 and b is 103.50371926281022\n",
      "Iteration 68, the loss is 11274.064511050003, parameters k is -10.531027031477578 and b is 108.04584926281022\n",
      "Iteration 69, the loss is 8418.77464694278, parameters k is -15.08078903147758 and b is 103.49608726281022\n",
      "Iteration 70, the loss is 11253.86689711156, parameters k is -10.538659031477582 and b is 108.03821726281022\n",
      "Iteration 71, the loss is 8364.290093885895, parameters k is -15.059881031477584 and b is 103.51699526281021\n",
      "Iteration 72, the loss is 11249.254695527537, parameters k is -10.540403031477585 and b is 108.0364732628102\n",
      "Iteration 73, the loss is 8368.817062701899, parameters k is -15.061625031477586 and b is 103.51525126281021\n",
      "Iteration 74, the loss is 11244.642493943536, parameters k is -10.542147031477587 and b is 108.0347292628102\n",
      "Iteration 75, the loss is 8373.344031517907, parameters k is -15.063369031477588 and b is 103.51350726281021\n",
      "Iteration 76, the loss is 11240.030292359512, parameters k is -10.543891031477589 and b is 108.0329852628102\n",
      "Iteration 77, the loss is 8377.874944002813, parameters k is -15.06511303147759 and b is 103.5117632628102\n",
      "Iteration 78, the loss is 11295.46863781396, parameters k is -10.52298303147759 and b is 108.0538932628102\n",
      "Iteration 79, the loss is 8397.787328082806, parameters k is -15.072745031477593 and b is 103.5041312628102\n",
      "Iteration 80, the loss is 11275.160794021956, parameters k is -10.530615031477595 and b is 108.0462612628102\n",
      "Iteration 81, the loss is 8417.699712162814, parameters k is -15.080377031477598 and b is 103.4964992628102\n",
      "Iteration 82, the loss is 11254.956476843508, parameters k is -10.5382470314776 and b is 108.0386292628102\n",
      "Iteration 83, the loss is 8363.220649417935, parameters k is -15.059469031477601 and b is 103.5174072628102\n",
      "Iteration 84, the loss is 11250.3442752595, parameters k is -10.539991031477602 and b is 108.03688526281019\n",
      "Iteration 85, the loss is 8367.747618233949, parameters k is -15.061213031477603 and b is 103.5156632628102\n",
      "Iteration 86, the loss is 11245.732073675495, parameters k is -10.541735031477604 and b is 108.03514126281019\n",
      "Iteration 87, the loss is 8372.274587049957, parameters k is -15.062957031477605 and b is 103.51391926281019\n",
      "Iteration 88, the loss is 11241.119872091489, parameters k is -10.543479031477606 and b is 108.03339726281018\n",
      "Iteration 89, the loss is 8376.801555865955, parameters k is -15.064701031477608 and b is 103.51217526281019\n",
      "Iteration 90, the loss is 11236.507670507477, parameters k is -10.545223031477608 and b is 108.03165326281018\n",
      "Iteration 91, the loss is 8381.350218582853, parameters k is -15.06644503147761 and b is 103.51043126281019\n",
      "Iteration 92, the loss is 11291.92434432192, parameters k is -10.52431503147761 and b is 108.05256126281019\n",
      "Iteration 93, the loss is 8401.262602662859, parameters k is -15.074077031477612 and b is 103.50279926281019\n",
      "Iteration 94, the loss is 11271.61752614346, parameters k is -10.531947031477614 and b is 108.04492926281019\n",
      "Iteration 95, the loss is 8346.867493717964, parameters k is -15.053169031477616 and b is 103.52370726281018\n",
      "Iteration 96, the loss is 11267.005324559457, parameters k is -10.533691031477616 and b is 108.04318526281017\n",
      "Iteration 97, the loss is 8351.39446253398, parameters k is -15.054913031477618 and b is 103.52196326281017\n",
      "Iteration 98, the loss is 11262.393122975458, parameters k is -10.535435031477618 and b is 108.04144126281017\n",
      "Iteration 99, the loss is 8355.921431349989, parameters k is -15.05665703147762 and b is 103.52021926281017\n",
      "Iteration 100, the loss is 11257.780921391439, parameters k is -10.53717903147762 and b is 108.03969726281017\n",
      "Iteration 101, the loss is 8360.448400165993, parameters k is -15.058401031477622 and b is 103.51847526281017\n",
      "Iteration 102, the loss is 11253.168719807447, parameters k is -10.538923031477623 and b is 108.03795326281016\n",
      "Iteration 103, the loss is 8364.975368981997, parameters k is -15.060145031477624 and b is 103.51673126281017\n",
      "Iteration 104, the loss is 11248.556518223437, parameters k is -10.540667031477625 and b is 108.03620926281016\n",
      "Iteration 105, the loss is 8369.502337798005, parameters k is -15.061889031477627 and b is 103.51498726281017\n",
      "Iteration 106, the loss is 11243.944316639429, parameters k is -10.542411031477627 and b is 108.03446526281016\n",
      "Iteration 107, the loss is 8374.029306614015, parameters k is -15.063633031477629 and b is 103.51324326281016\n",
      "Iteration 108, the loss is 11239.332115055442, parameters k is -10.54415503147763 and b is 108.03272126281016\n",
      "Iteration 109, the loss is 8378.563737162913, parameters k is -15.065377031477631 and b is 103.51149926281016\n",
      "Iteration 110, the loss is 11294.766165229847, parameters k is -10.52324703147763 and b is 108.05362926281016\n",
      "Iteration 111, the loss is 8398.476121242917, parameters k is -15.073009031477634 and b is 103.50386726281016\n",
      "Iteration 112, the loss is 11274.45832143785, parameters k is -10.530879031477635 and b is 108.04599726281016\n",
      "Iteration 113, the loss is 8418.388505322931, parameters k is -15.080641031477638 and b is 103.49623526281016\n",
      "Iteration 114, the loss is 11254.25829953939, parameters k is -10.53851103147764 and b is 108.03836526281016\n",
      "Iteration 115, the loss is 8363.905924514043, parameters k is -15.059733031477641 and b is 103.51714326281015\n",
      "Iteration 116, the loss is 11249.646097955387, parameters k is -10.540255031477642 and b is 108.03662126281014\n",
      "Iteration 117, the loss is 8368.432893330053, parameters k is -15.061477031477644 and b is 103.51539926281015\n",
      "Iteration 118, the loss is 11245.033896371382, parameters k is -10.541999031477644 and b is 108.03487726281014\n",
      "Iteration 119, the loss is 8372.95986214606, parameters k is -15.063221031477646 and b is 103.51365526281015\n",
      "Iteration 120, the loss is 11240.421694787387, parameters k is -10.543743031477646 and b is 108.03313326281014\n",
      "Iteration 121, the loss is 8377.488802382955, parameters k is -15.064965031477648 and b is 103.51191126281014\n",
      "Iteration 122, the loss is 11295.862448201819, parameters k is -10.522835031477648 and b is 108.05404126281014\n",
      "Iteration 123, the loss is 8397.401186462963, parameters k is -15.07259703147765 and b is 103.50427926281014\n",
      "Iteration 124, the loss is 11275.554604409801, parameters k is -10.530467031477652 and b is 108.04640926281014\n",
      "Iteration 125, the loss is 8417.31357054297, parameters k is -15.080229031477655 and b is 103.49664726281014\n",
      "Iteration 126, the loss is 11255.347879271354, parameters k is -10.538099031477657 and b is 108.03877726281014\n",
      "Iteration 127, the loss is 8362.836480046095, parameters k is -15.059321031477658 and b is 103.51755526281013\n",
      "Iteration 128, the loss is 11250.73567768735, parameters k is -10.539843031477659 and b is 108.03703326281013\n",
      "Iteration 129, the loss is 8367.363448862094, parameters k is -15.06106503147766 and b is 103.51581126281013\n",
      "Iteration 130, the loss is 11246.123476103343, parameters k is -10.541587031477661 and b is 108.03528926281012\n",
      "Iteration 131, the loss is 8371.890417678102, parameters k is -15.062809031477663 and b is 103.51406726281013\n",
      "Iteration 132, the loss is 11241.511274519336, parameters k is -10.543331031477663 and b is 108.03354526281012\n",
      "Iteration 133, the loss is 8376.417386494102, parameters k is -15.064553031477665 and b is 103.51232326281013\n",
      "Iteration 134, the loss is 11236.899072935334, parameters k is -10.545075031477666 and b is 108.03180126281012\n",
      "Iteration 135, the loss is 8380.964076963002, parameters k is -15.066297031477667 and b is 103.51057926281013\n",
      "Iteration 136, the loss is 11292.31815470977, parameters k is -10.524167031477667 and b is 108.05270926281013\n",
      "Iteration 137, the loss is 8400.87646104301, parameters k is -15.07392903147767 and b is 103.50294726281012\n",
      "Iteration 138, the loss is 11272.010310917743, parameters k is -10.531799031477671 and b is 108.04507726281012\n",
      "Iteration 139, the loss is 8420.788845123021, parameters k is -15.081561031477674 and b is 103.49531526281012\n",
      "Iteration 140, the loss is 11251.825257419308, parameters k is -10.539431031477676 and b is 108.03744526281012\n",
      "Iteration 141, the loss is 8366.29400439413, parameters k is -15.060653031477678 and b is 103.51622326281012\n",
      "Iteration 142, the loss is 11247.213055835304, parameters k is -10.541175031477678 and b is 108.03570126281011\n",
      "Iteration 143, the loss is 8370.82097321015, parameters k is -15.06239703147768 and b is 103.51447926281011\n",
      "Iteration 144, the loss is 11242.600854251286, parameters k is -10.54291903147768 and b is 108.0339572628101\n",
      "Iteration 145, the loss is 8375.347942026148, parameters k is -15.064141031477682 and b is 103.51273526281011\n",
      "Iteration 146, the loss is 11237.988652667287, parameters k is -10.544663031477683 and b is 108.0322132628101\n",
      "Iteration 147, the loss is 8379.889142183047, parameters k is -15.065885031477684 and b is 103.51099126281011\n",
      "Iteration 148, the loss is 11293.414437681708, parameters k is -10.523755031477684 and b is 108.05312126281011\n",
      "Iteration 149, the loss is 8399.801526263062, parameters k is -15.073517031477687 and b is 103.50335926281011\n",
      "Iteration 150, the loss is 11273.106593889697, parameters k is -10.531387031477689 and b is 108.04548926281011\n",
      "Iteration 151, the loss is 8419.713910343078, parameters k is -15.081149031477691 and b is 103.4957272628101\n",
      "Iteration 152, the loss is 11252.914837151258, parameters k is -10.539019031477693 and b is 108.0378572628101\n",
      "Iteration 153, the loss is 8365.224559926171, parameters k is -15.060241031477695 and b is 103.5166352628101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 154, the loss is 11248.302635567248, parameters k is -10.540763031477695 and b is 108.03611326281009\n",
      "Iteration 155, the loss is 8369.751528742188, parameters k is -15.061985031477697 and b is 103.5148912628101\n",
      "Iteration 156, the loss is 11243.690433983256, parameters k is -10.542507031477697 and b is 108.03436926281009\n",
      "Iteration 157, the loss is 8374.2784975582, parameters k is -15.063729031477699 and b is 103.5131472628101\n",
      "Iteration 158, the loss is 11239.078232399233, parameters k is -10.5442510314777 and b is 108.03262526281009\n",
      "Iteration 159, the loss is 8378.814207403095, parameters k is -15.065473031477701 and b is 103.51140326281009\n",
      "Iteration 160, the loss is 11294.51072065367, parameters k is -10.523343031477701 and b is 108.05353326281009\n",
      "Iteration 161, the loss is 8398.726591483106, parameters k is -15.073105031477704 and b is 103.50377126281009\n",
      "Iteration 162, the loss is 11274.202876861666, parameters k is -10.530975031477706 and b is 108.04590126281009\n",
      "Iteration 163, the loss is 8418.6389755631, parameters k is -15.080737031477708 and b is 103.49613926281009\n",
      "Iteration 164, the loss is 11254.004416883217, parameters k is -10.53860703147771 and b is 108.03826926281009\n",
      "Iteration 165, the loss is 8364.155115458212, parameters k is -15.059829031477712 and b is 103.51704726281008\n",
      "Iteration 166, the loss is 11249.3922152992, parameters k is -10.540351031477712 and b is 108.03652526281007\n",
      "Iteration 167, the loss is 8368.682084274227, parameters k is -15.061573031477714 and b is 103.51530326281008\n",
      "Iteration 168, the loss is 11244.780013715197, parameters k is -10.542095031477714 and b is 108.03478126281007\n",
      "Iteration 169, the loss is 8373.209053090231, parameters k is -15.063317031477716 and b is 103.51355926281008\n",
      "Iteration 170, the loss is 11240.167812131203, parameters k is -10.543839031477717 and b is 108.03303726281007\n",
      "Iteration 171, the loss is 8377.739272623136, parameters k is -15.065061031477718 and b is 103.51181526281007\n",
      "Iteration 172, the loss is 11295.60700362563, parameters k is -10.522931031477718 and b is 108.05394526281007\n",
      "Iteration 173, the loss is 8397.651656703152, parameters k is -15.072693031477721 and b is 103.50418326281007\n",
      "Iteration 174, the loss is 11275.299159833608, parameters k is -10.530563031477723 and b is 108.04631326281007\n",
      "Iteration 175, the loss is 8417.564040783149, parameters k is -15.080325031477726 and b is 103.49655126281007\n",
      "Iteration 176, the loss is 11255.093996615167, parameters k is -10.538195031477727 and b is 108.03868126281007\n",
      "Iteration 177, the loss is 8363.085670990267, parameters k is -15.059417031477729 and b is 103.51745926281006\n",
      "Iteration 178, the loss is 11250.48179503115, parameters k is -10.53993903147773 and b is 108.03693726281006\n",
      "Iteration 179, the loss is 8367.612639806279, parameters k is -15.06116103147773 and b is 103.51571526281006\n",
      "Iteration 180, the loss is 11245.869593447156, parameters k is -10.541683031477731 and b is 108.03519326281005\n",
      "Iteration 181, the loss is 8372.139608622278, parameters k is -15.062905031477733 and b is 103.51397126281006\n",
      "Iteration 182, the loss is 11241.257391863159, parameters k is -10.543427031477734 and b is 108.03344926281005\n",
      "Iteration 183, the loss is 8376.666577438285, parameters k is -15.064649031477735 and b is 103.51222726281006\n",
      "Iteration 184, the loss is 11236.645190279152, parameters k is -10.545171031477736 and b is 108.03170526281005\n",
      "Iteration 185, the loss is 8381.214547203192, parameters k is -15.066393031477737 and b is 103.51048326281006\n",
      "Iteration 186, the loss is 11292.062710133581, parameters k is -10.524263031477737 and b is 108.05261326281006\n",
      "Iteration 187, the loss is 8401.126931283196, parameters k is -15.07402503147774 and b is 103.50285126281005\n",
      "Iteration 188, the loss is 11271.755045915135, parameters k is -10.531895031477742 and b is 108.04498126281005\n",
      "Iteration 189, the loss is 8346.732515290303, parameters k is -15.053117031477743 and b is 103.52375926281005\n",
      "Iteration 190, the loss is 11267.142844331109, parameters k is -10.533639031477744 and b is 108.04323726281004\n",
      "Iteration 191, the loss is 8351.259484106311, parameters k is -15.054861031477746 and b is 103.52201526281004\n",
      "Iteration 192, the loss is 11262.530642747115, parameters k is -10.535383031477746 and b is 108.04149326281004\n",
      "Iteration 193, the loss is 8355.78645292232, parameters k is -15.056605031477748 and b is 103.52027126281004\n",
      "Iteration 194, the loss is 11257.918441163107, parameters k is -10.537127031477748 and b is 108.03974926281003\n",
      "Iteration 195, the loss is 8360.313421738327, parameters k is -15.05834903147775 and b is 103.51852726281004\n",
      "Iteration 196, the loss is 11253.3062395791, parameters k is -10.53887103147775 and b is 108.03800526281003\n",
      "Iteration 197, the loss is 8364.840390554335, parameters k is -15.060093031477752 and b is 103.51678326281004\n",
      "Iteration 198, the loss is 11248.694037995108, parameters k is -10.540615031477753 and b is 108.03626126281003\n",
      "Iteration 199, the loss is 8369.36735937034, parameters k is -15.061837031477754 and b is 103.51503926281003\n",
      "Iteration 0, the loss is 92855.62097206147, parameters k is 43.291038139164755 and b is -66.02640827358854\n",
      "Iteration 1, the loss is 81133.96932143663, parameters k is 40.111013139164754 and b is -69.20643327358854\n",
      "Iteration 2, the loss is 69412.31767081162, parameters k is 36.930988139164754 and b is -72.38645827358854\n",
      "Iteration 3, the loss is 57690.66602018661, parameters k is 33.75096313916475 and b is -75.56648327358855\n",
      "Iteration 4, the loss is 45969.014369561606, parameters k is 30.57093813916475 and b is -78.74650827358855\n",
      "Iteration 5, the loss is 34271.13770210662, parameters k is 27.390913139164745 and b is -81.92653327358855\n",
      "Iteration 6, the loss is 22663.443520673445, parameters k is 24.21801013916474 and b is -85.09943627358855\n",
      "Iteration 7, the loss is 11270.153726960532, parameters k is 21.062773139164737 and b is -88.25467327358855\n",
      "Iteration 8, the loss is 3170.4627336974454, parameters k is 17.99453013916473 and b is -91.32291627358856\n",
      "Iteration 9, the loss is 3174.1668178863524, parameters k is 18.07724113916473 and b is -91.24020527358856\n",
      "Iteration 10, the loss is 3246.449086555545, parameters k is 17.83547813916473 and b is -91.48196827358856\n",
      "Iteration 11, the loss is 3449.3108978487003, parameters k is 18.450831139164734 and b is -90.86661527358856\n",
      "Iteration 12, the loss is 3915.430333845882, parameters k is 17.358936139164737 and b is -91.95851027358856\n",
      "Iteration 13, the loss is 4689.224564356064, parameters k is 19.056691139164737 and b is -90.26075527358856\n",
      "Iteration 14, the loss is 5584.001562505832, parameters k is 16.702162139164734 and b is -92.61528427358856\n",
      "Iteration 15, the loss is 5254.951496956378, parameters k is 19.256667139164737 and b is -90.06077927358855\n",
      "Iteration 16, the loss is 5670.936479501706, parameters k is 16.673058139164734 and b is -92.64438827358855\n",
      "Iteration 17, the loss is 5244.177467812747, parameters k is 19.253047139164735 and b is -90.06439927358855\n",
      "Iteration 18, the loss is 5646.468569217706, parameters k is 16.681214139164734 and b is -92.63623227358855\n",
      "Iteration 19, the loss is 5268.457931380371, parameters k is 19.261203139164735 and b is -90.05624327358855\n",
      "Iteration 20, the loss is 5657.328529397711, parameters k is 16.677594139164732 and b is -92.63985227358855\n",
      "Iteration 21, the loss is 5257.678986800369, parameters k is 19.257583139164733 and b is -90.05986327358855\n",
      "Iteration 22, the loss is 5668.188489577713, parameters k is 16.67397413916473 and b is -92.64347227358854\n",
      "Iteration 23, the loss is 5246.900042220361, parameters k is 19.253963139164732 and b is -90.06348327358855\n",
      "Iteration 24, the loss is 5679.04844975772, parameters k is 16.67035413916473 and b is -92.64709227358854\n",
      "Iteration 25, the loss is 5236.16326338074, parameters k is 19.25034313916473 and b is -90.06710327358854\n",
      "Iteration 26, the loss is 5654.580539473717, parameters k is 16.67851013916473 and b is -92.63893627358854\n",
      "Iteration 27, the loss is 5260.406476644356, parameters k is 19.25849913916473 and b is -90.05894727358854\n",
      "Iteration 28, the loss is 5665.4404996537205, parameters k is 16.674890139164727 and b is -92.64255627358854\n",
      "Iteration 29, the loss is 5249.62753206436, parameters k is 19.25487913916473 and b is -90.06256727358854\n",
      "Iteration 30, the loss is 5676.300459833727, parameters k is 16.671270139164726 and b is -92.64617627358854\n",
      "Iteration 31, the loss is 5238.878134408724, parameters k is 19.251259139164727 and b is -90.06618727358854\n",
      "Iteration 32, the loss is 5651.832549549727, parameters k is 16.679426139164725 and b is -92.63802027358854\n",
      "Iteration 33, the loss is 5263.133966488361, parameters k is 19.259415139164727 and b is -90.05803127358854\n",
      "Iteration 34, the loss is 5662.692509729731, parameters k is 16.675806139164724 and b is -92.64164027358854\n",
      "Iteration 35, the loss is 5252.355021908351, parameters k is 19.255795139164725 and b is -90.06165127358854\n",
      "Iteration 36, the loss is 5673.552469909734, parameters k is 16.672186139164722 and b is -92.64526027358853\n",
      "Iteration 37, the loss is 5241.59300543672, parameters k is 19.252175139164724 and b is -90.06527127358854\n",
      "Iteration 38, the loss is 5649.0845596257395, parameters k is 16.680342139164722 and b is -92.63710427358853\n",
      "Iteration 39, the loss is 5265.86145633235, parameters k is 19.260331139164723 and b is -90.05711527358854\n",
      "Iteration 40, the loss is 5659.944519805736, parameters k is 16.67672213916472 and b is -92.64072427358853\n",
      "Iteration 41, the loss is 5255.082511752347, parameters k is 19.25671113916472 and b is -90.06073527358853\n",
      "Iteration 42, the loss is 5670.804479985739, parameters k is 16.67310213916472 and b is -92.64434427358853\n",
      "Iteration 43, the loss is 5244.307876464713, parameters k is 19.25309113916472 and b is -90.06435527358853\n",
      "Iteration 44, the loss is 5646.336569701737, parameters k is 16.68125813916472 and b is -92.63618827358853\n",
      "Iteration 45, the loss is 5268.588946176342, parameters k is 19.26124713916472 and b is -90.05619927358853\n",
      "Iteration 46, the loss is 5657.196529881739, parameters k is 16.677638139164717 and b is -92.63980827358853\n",
      "Iteration 47, the loss is 5257.810001596343, parameters k is 19.25762713916472 and b is -90.05981927358853\n",
      "Iteration 48, the loss is 5668.056490061744, parameters k is 16.674018139164716 and b is -92.64342827358853\n",
      "Iteration 49, the loss is 5247.031057016336, parameters k is 19.254007139164717 and b is -90.06343927358853\n",
      "Iteration 50, the loss is 5678.916450241747, parameters k is 16.670398139164714 and b is -92.64704827358852\n",
      "Iteration 51, the loss is 5236.293672032706, parameters k is 19.250387139164715 and b is -90.06705927358853\n",
      "Iteration 52, the loss is 5654.448539957749, parameters k is 16.678554139164714 and b is -92.63889227358852\n",
      "Iteration 53, the loss is 5260.537491440329, parameters k is 19.258543139164715 and b is -90.05890327358853\n",
      "Iteration 54, the loss is 5665.308500137749, parameters k is 16.674934139164712 and b is -92.64251227358852\n",
      "Iteration 55, the loss is 5249.7585468603265, parameters k is 19.254923139164713 and b is -90.06252327358852\n",
      "Iteration 56, the loss is 5676.168460317757, parameters k is 16.67131413916471 and b is -92.64613227358852\n",
      "Iteration 57, the loss is 5239.008543060693, parameters k is 19.251303139164712 and b is -90.06614327358852\n",
      "Iteration 58, the loss is 5651.700550033764, parameters k is 16.67947013916471 and b is -92.63797627358852\n",
      "Iteration 59, the loss is 5263.264981284323, parameters k is 19.25945913916471 and b is -90.05798727358852\n",
      "Iteration 60, the loss is 5662.560510213762, parameters k is 16.67585013916471 and b is -92.64159627358852\n",
      "Iteration 61, the loss is 5252.486036704317, parameters k is 19.25583913916471 and b is -90.06160727358852\n",
      "Iteration 62, the loss is 5673.420470393769, parameters k is 16.672230139164707 and b is -92.64521627358852\n",
      "Iteration 63, the loss is 5241.723414088697, parameters k is 19.25221913916471 and b is -90.06522727358852\n",
      "Iteration 64, the loss is 5648.952560109771, parameters k is 16.680386139164707 and b is -92.63706027358852\n",
      "Iteration 65, the loss is 5265.992471128312, parameters k is 19.260375139164708 and b is -90.05707127358852\n",
      "Iteration 66, the loss is 5659.812520289768, parameters k is 16.676766139164705 and b is -92.64068027358852\n",
      "Iteration 67, the loss is 5255.2135265483075, parameters k is 19.256755139164706 and b is -90.06069127358852\n",
      "Iteration 68, the loss is 5670.672480469772, parameters k is 16.673146139164704 and b is -92.64430027358851\n",
      "Iteration 69, the loss is 5244.438285116683, parameters k is 19.253135139164705 and b is -90.06431127358852\n",
      "Iteration 70, the loss is 5646.204570185776, parameters k is 16.681302139164703 and b is -92.63614427358851\n",
      "Iteration 71, the loss is 5268.719960972306, parameters k is 19.261291139164705 and b is -90.05615527358852\n",
      "Iteration 72, the loss is 5657.064530365777, parameters k is 16.677682139164702 and b is -92.63976427358851\n",
      "Iteration 73, the loss is 5257.941016392302, parameters k is 19.257671139164703 and b is -90.05977527358851\n",
      "Iteration 74, the loss is 5667.924490545777, parameters k is 16.6740621391647 and b is -92.64338427358851\n",
      "Iteration 75, the loss is 5247.162071812306, parameters k is 19.2540511391647 and b is -90.06339527358851\n",
      "Iteration 76, the loss is 5678.784450725774, parameters k is 16.6704421391647 and b is -92.64700427358851\n",
      "Iteration 77, the loss is 5236.424080684669, parameters k is 19.2504311391647 and b is -90.06701527358851\n",
      "Iteration 78, the loss is 5654.316540441788, parameters k is 16.6785981391647 and b is -92.63884827358851\n",
      "Iteration 79, the loss is 5260.668506236301, parameters k is 19.2585871391647 and b is -90.05885927358851\n",
      "Iteration 80, the loss is 5665.176500621784, parameters k is 16.674978139164697 and b is -92.6424682735885\n",
      "Iteration 81, the loss is 5249.889561656297, parameters k is 19.254967139164698 and b is -90.06247927358851\n",
      "Iteration 82, the loss is 5676.036460801786, parameters k is 16.671358139164695 and b is -92.6460882735885\n",
      "Iteration 83, the loss is 5239.1389517126645, parameters k is 19.251347139164697 and b is -90.0660992735885\n",
      "Iteration 84, the loss is 5651.568550517792, parameters k is 16.679514139164695 and b is -92.6379322735885\n",
      "Iteration 85, the loss is 5263.395996080294, parameters k is 19.259503139164696 and b is -90.0579432735885\n",
      "Iteration 86, the loss is 5662.428510697793, parameters k is 16.675894139164694 and b is -92.6415522735885\n",
      "Iteration 87, the loss is 5252.617051500284, parameters k is 19.255883139164695 and b is -90.0615632735885\n",
      "Iteration 88, the loss is 5673.288470877799, parameters k is 16.672274139164692 and b is -92.6451722735885\n",
      "Iteration 89, the loss is 5241.853822740657, parameters k is 19.252263139164693 and b is -90.0651832735885\n",
      "Iteration 90, the loss is 5648.820560593793, parameters k is 16.68043013916469 and b is -92.6370162735885\n",
      "Iteration 91, the loss is 5266.123485924282, parameters k is 19.260419139164693 and b is -90.0570272735885\n",
      "Iteration 92, the loss is 5659.6805207738025, parameters k is 16.67681013916469 and b is -92.6406362735885\n",
      "Iteration 93, the loss is 5255.344541344277, parameters k is 19.25679913916469 and b is -90.0606472735885\n",
      "Iteration 94, the loss is 5670.5404809537995, parameters k is 16.67319013916469 and b is -92.6442562735885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, the loss is 5244.568693768644, parameters k is 19.25317913916469 and b is -90.0642672735885\n",
      "Iteration 96, the loss is 5646.072570669806, parameters k is 16.68134613916469 and b is -92.6361002735885\n",
      "Iteration 97, the loss is 5268.850975768272, parameters k is 19.26133513916469 and b is -90.0561112735885\n",
      "Iteration 98, the loss is 5656.932530849803, parameters k is 16.677726139164687 and b is -92.6397202735885\n",
      "Iteration 99, the loss is 5258.0720311882715, parameters k is 19.257715139164688 and b is -90.0597312735885\n",
      "Iteration 100, the loss is 5667.792491029817, parameters k is 16.674106139164685 and b is -92.64334027358849\n",
      "Iteration 101, the loss is 5247.29308660827, parameters k is 19.254095139164686 and b is -90.0633512735885\n",
      "Iteration 102, the loss is 5678.652451209813, parameters k is 16.670486139164684 and b is -92.64696027358849\n",
      "Iteration 103, the loss is 5236.554489336638, parameters k is 19.250475139164685 and b is -90.06697127358849\n",
      "Iteration 104, the loss is 5654.184540925817, parameters k is 16.678642139164683 and b is -92.63880427358849\n",
      "Iteration 105, the loss is 5260.799521032264, parameters k is 19.258631139164684 and b is -90.0588152735885\n",
      "Iteration 106, the loss is 5665.044501105815, parameters k is 16.675022139164682 and b is -92.64242427358849\n",
      "Iteration 107, the loss is 5250.020576452263, parameters k is 19.255011139164683 and b is -90.06243527358849\n",
      "Iteration 108, the loss is 5675.9044612858215, parameters k is 16.67140213916468 and b is -92.64604427358849\n",
      "Iteration 109, the loss is 5239.269360364637, parameters k is 19.25139113916468 and b is -90.06605527358849\n",
      "Iteration 110, the loss is 5651.436551001821, parameters k is 16.67955813916468 and b is -92.63788827358849\n",
      "Iteration 111, the loss is 5263.527010876259, parameters k is 19.25954713916468 and b is -90.05789927358849\n",
      "Iteration 112, the loss is 5662.296511181826, parameters k is 16.67593813916468 and b is -92.64150827358849\n",
      "Iteration 113, the loss is 5252.748066296254, parameters k is 19.25592713916468 and b is -90.06151927358849\n",
      "Iteration 114, the loss is 5673.156471361824, parameters k is 16.672318139164677 and b is -92.64512827358848\n",
      "Iteration 115, the loss is 5241.98423139263, parameters k is 19.252307139164678 and b is -90.06513927358849\n",
      "Iteration 116, the loss is 5648.6885610778245, parameters k is 16.680474139164676 and b is -92.63697227358848\n",
      "Iteration 117, the loss is 5266.254500720251, parameters k is 19.260463139164678 and b is -90.05698327358849\n",
      "Iteration 118, the loss is 5659.54852125784, parameters k is 16.676854139164675 and b is -92.64059227358848\n",
      "Iteration 119, the loss is 5255.475556140247, parameters k is 19.256843139164676 and b is -90.06060327358848\n",
      "Iteration 120, the loss is 5670.408481437837, parameters k is 16.673234139164673 and b is -92.64421227358848\n",
      "Iteration 121, the loss is 5244.699102420615, parameters k is 19.253223139164675 and b is -90.06422327358848\n",
      "Iteration 122, the loss is 5645.940571153838, parameters k is 16.681390139164673 and b is -92.63605627358848\n",
      "Iteration 123, the loss is 5268.982082978409, parameters k is 19.261379139164674 and b is -90.05606727358848\n",
      "Iteration 124, the loss is 5691.924402545837, parameters k is 16.666062139164673 and b is -92.65138427358849\n",
      "Iteration 125, the loss is 5223.529620050798, parameters k is 19.246051139164674 and b is -90.0713952735885\n",
      "Iteration 126, the loss is 5600.024466475968, parameters k is 16.69676813916467 and b is -92.6206782735885\n",
      "Iteration 127, the loss is 5238.919628070615, parameters k is 19.251273139164674 and b is -90.06617327358849\n",
      "Iteration 128, the loss is 5651.790549703834, parameters k is 16.679440139164672 and b is -92.63800627358849\n",
      "Iteration 129, the loss is 5263.175653014239, parameters k is 19.259429139164673 and b is -90.05801727358849\n",
      "Iteration 130, the loss is 5662.650509883846, parameters k is 16.67582013916467 and b is -92.64162627358849\n",
      "Iteration 131, the loss is 5252.396708434236, parameters k is 19.255809139164672 and b is -90.06163727358849\n",
      "Iteration 132, the loss is 5673.510470063848, parameters k is 16.67220013916467 and b is -92.64524627358848\n",
      "Iteration 133, the loss is 5241.6344990986045, parameters k is 19.25218913916467 and b is -90.06525727358849\n",
      "Iteration 134, the loss is 5649.042559779854, parameters k is 16.68035613916467 and b is -92.63709027358848\n",
      "Iteration 135, the loss is 5265.903142858228, parameters k is 19.26034513916467 and b is -90.05710127358849\n",
      "Iteration 136, the loss is 5659.902519959852, parameters k is 16.676736139164667 and b is -92.64071027358848\n",
      "Iteration 137, the loss is 5255.1241982782285, parameters k is 19.25672513916467 and b is -90.06072127358848\n",
      "Iteration 138, the loss is 5670.76248013986, parameters k is 16.673116139164666 and b is -92.64433027358848\n",
      "Iteration 139, the loss is 5244.349370126596, parameters k is 19.253105139164667 and b is -90.06434127358848\n",
      "Iteration 140, the loss is 5646.29456985586, parameters k is 16.681272139164665 and b is -92.63617427358848\n",
      "Iteration 141, the loss is 5268.630632702222, parameters k is 19.261261139164667 and b is -90.05618527358848\n",
      "Iteration 142, the loss is 5657.154530035858, parameters k is 16.677652139164664 and b is -92.63979427358848\n",
      "Iteration 143, the loss is 5257.8516881222195, parameters k is 19.257641139164665 and b is -90.05980527358848\n",
      "Iteration 144, the loss is 5668.014490215858, parameters k is 16.674032139164662 and b is -92.64341427358848\n",
      "Iteration 145, the loss is 5247.072743542216, parameters k is 19.254021139164664 and b is -90.06342527358848\n",
      "Iteration 146, the loss is 5678.874450395865, parameters k is 16.67041213916466 and b is -92.64703427358847\n",
      "Iteration 147, the loss is 5236.335165694589, parameters k is 19.250401139164662 and b is -90.06704527358848\n",
      "Iteration 148, the loss is 5654.406540111865, parameters k is 16.67856813916466 and b is -92.63887827358847\n",
      "Iteration 149, the loss is 5260.5791779662095, parameters k is 19.25855713916466 and b is -90.05888927358848\n",
      "Iteration 150, the loss is 5665.266500291867, parameters k is 16.67494813916466 and b is -92.64249827358847\n",
      "Iteration 151, the loss is 5249.800233386207, parameters k is 19.25493713916466 and b is -90.06250927358847\n",
      "Iteration 152, the loss is 5676.126460471864, parameters k is 16.671328139164658 and b is -92.64611827358847\n",
      "Iteration 153, the loss is 5239.050036722579, parameters k is 19.25131713916466 and b is -90.06612927358847\n",
      "Iteration 154, the loss is 5651.658550187874, parameters k is 16.679484139164657 and b is -92.63796227358847\n",
      "Iteration 155, the loss is 5263.306667810203, parameters k is 19.25947313916466 and b is -90.05797327358847\n",
      "Iteration 156, the loss is 5662.518510367873, parameters k is 16.675864139164656 and b is -92.64158227358847\n",
      "Iteration 157, the loss is 5252.527723230203, parameters k is 19.255853139164657 and b is -90.06159327358847\n",
      "Iteration 158, the loss is 5673.378470547879, parameters k is 16.672244139164654 and b is -92.64520227358847\n",
      "Iteration 159, the loss is 5241.764907750577, parameters k is 19.252233139164655 and b is -90.06521327358847\n",
      "Iteration 160, the loss is 5648.910560263884, parameters k is 16.680400139164654 and b is -92.63704627358847\n",
      "Iteration 161, the loss is 5266.034157654195, parameters k is 19.260389139164655 and b is -90.05705727358847\n",
      "Iteration 162, the loss is 5659.770520443889, parameters k is 16.676780139164652 and b is -92.64066627358847\n",
      "Iteration 163, the loss is 5255.255213074191, parameters k is 19.256769139164653 and b is -90.06067727358847\n",
      "Iteration 164, the loss is 5670.630480623886, parameters k is 16.67316013916465 and b is -92.64428627358846\n",
      "Iteration 165, the loss is 5244.479778778568, parameters k is 19.253149139164652 and b is -90.06429727358847\n",
      "Iteration 166, the loss is 5646.162570339889, parameters k is 16.68131613916465 and b is -92.63613027358846\n",
      "Iteration 167, the loss is 5268.761647498186, parameters k is 19.26130513916465 and b is -90.05614127358847\n",
      "Iteration 168, the loss is 5657.022530519886, parameters k is 16.67769613916465 and b is -92.63975027358846\n",
      "Iteration 169, the loss is 5257.982702918188, parameters k is 19.25768513916465 and b is -90.05976127358846\n",
      "Iteration 170, the loss is 5667.882490699898, parameters k is 16.674076139164647 and b is -92.64337027358846\n",
      "Iteration 171, the loss is 5247.2037583381825, parameters k is 19.25406513916465 and b is -90.06338127358846\n",
      "Iteration 172, the loss is 5678.742450879897, parameters k is 16.670456139164646 and b is -92.64699027358846\n",
      "Iteration 173, the loss is 5236.465574346556, parameters k is 19.250445139164647 and b is -90.06700127358846\n",
      "Iteration 174, the loss is 5654.274540595902, parameters k is 16.678612139164645 and b is -92.63883427358846\n",
      "Iteration 175, the loss is 5260.710192762181, parameters k is 19.258601139164647 and b is -90.05884527358846\n",
      "Iteration 176, the loss is 5665.1345007758955, parameters k is 16.674992139164644 and b is -92.64245427358846\n",
      "Iteration 177, the loss is 5249.931248182179, parameters k is 19.254981139164645 and b is -90.06246527358846\n",
      "Iteration 178, the loss is 5675.994460955908, parameters k is 16.671372139164642 and b is -92.64607427358845\n",
      "Iteration 179, the loss is 5239.180445374542, parameters k is 19.251361139164644 and b is -90.06608527358846\n",
      "Iteration 180, the loss is 5651.5265506719, parameters k is 16.679528139164642 and b is -92.63791827358845\n",
      "Iteration 181, the loss is 5263.437682606169, parameters k is 19.259517139164643 and b is -90.05792927358846\n",
      "Iteration 182, the loss is 5662.3865108519085, parameters k is 16.67590813916464 and b is -92.64153827358845\n",
      "Iteration 183, the loss is 5252.658738026168, parameters k is 19.25589713916464 and b is -90.06154927358845\n",
      "Iteration 184, the loss is 5673.246471031913, parameters k is 16.67228813916464 and b is -92.64515827358845\n",
      "Iteration 185, the loss is 5241.89531640254, parameters k is 19.25227713916464 and b is -90.06516927358845\n",
      "Iteration 186, the loss is 5648.778560747909, parameters k is 16.68044413916464 and b is -92.63700227358845\n",
      "Iteration 187, the loss is 5266.165172450167, parameters k is 19.26043313916464 and b is -90.05701327358845\n",
      "Iteration 188, the loss is 5659.638520927911, parameters k is 16.676824139164637 and b is -92.64062227358845\n",
      "Iteration 189, the loss is 5255.386227870165, parameters k is 19.256813139164638 and b is -90.06063327358845\n",
      "Iteration 190, the loss is 5670.498481107923, parameters k is 16.673204139164636 and b is -92.64424227358845\n",
      "Iteration 191, the loss is 5244.61018743053, parameters k is 19.253193139164637 and b is -90.06425327358845\n",
      "Iteration 192, the loss is 5646.030570823915, parameters k is 16.681360139164635 and b is -92.63608627358845\n",
      "Iteration 193, the loss is 5268.8926622941535, parameters k is 19.261349139164636 and b is -90.05609727358845\n",
      "Iteration 194, the loss is 5656.890531003918, parameters k is 16.677740139164634 and b is -92.63970627358844\n",
      "Iteration 195, the loss is 5258.113717714154, parameters k is 19.257729139164635 and b is -90.05971727358845\n",
      "Iteration 196, the loss is 5667.750491183925, parameters k is 16.674120139164632 and b is -92.64332627358844\n",
      "Iteration 197, the loss is 5247.334773134154, parameters k is 19.254109139164633 and b is -90.06333727358844\n",
      "Iteration 198, the loss is 5678.61045136393, parameters k is 16.67050013916463 and b is -92.64694627358844\n",
      "Iteration 199, the loss is 5236.595982998528, parameters k is 19.250489139164632 and b is -90.06695727358844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16.67865613916463, -92.63879027358844)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]\n",
    "\n",
    "def target_func(x, k, b):\n",
    "    return (k*x+b)\n",
    "\n",
    "\n",
    "def loss_func(y, y_hat):\n",
    "    #return abs(sum((y_i - y_hat_i)**2 for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y)))\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))\n",
    "                                                                                                                                                                        \n",
    "def partial_derivative_k(x, y, y_hat):\n",
    "    gradient = 0\n",
    "    k_gradie = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            k_gradie = -x_i\n",
    "        else:\n",
    "            k_gradie = x_i\n",
    "        gradient += k_gradie\n",
    "    return gradient\n",
    "                                                                                    \n",
    "\n",
    "def partial_derivative_b(x, y, y_hat):                                                                                 \n",
    "    gradient = 0\n",
    "    b_gradie = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            b_gradie = -1\n",
    "        else:\n",
    "            b_gradie = 1\n",
    "        gradient += b_gradie\n",
    "    return gradient\n",
    "\n",
    "def linear_regression(data_x, data_y, learning_rate, iteration_num):\n",
    "    k = random.random()*200 - 100\n",
    "    b = random.random()*200 - 100\n",
    "    losses=[]\n",
    "    for i in range(iteration_num):\n",
    "        price_use_current_y_hat = [target_func(x_i, k, b) for x_i in data_x]\n",
    "        current_loss = loss_func(data_y, price_use_current_y_hat)\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "        \n",
    "        k_gradient = partial_derivative_k(data_x, data_y, price_use_current_y_hat)\n",
    "        b_gradient = partial_derivative_b(data_x, data_y, price_use_current_y_hat)\n",
    "        \n",
    "        k = k - learning_rate * k_gradient\n",
    "        b = b - learning_rate * k_gradient\n",
    "        \n",
    "    plt.plot(list(range(iteration_num)),losses)\n",
    "    return k, b\n",
    "\n",
    "linear_regression(X_rm, y, 0.02, 200)\n",
    "linear_regression(X_rm, y, 0.01, 200)\n",
    "linear_regression(X_rm, y, 0.002, 200)\n",
    "linear_regression(X_rm, y, 0.001, 200)\n",
    "\n",
    "##Loss function不能完全收敛，但是当learning_rate较小时，Loss function会不断减小，最终在某个值上下很小范围\n",
    "##内来回震荡波动，但此时k和b的取值并不唯一;learning_rate越大，其震荡幅度会不断增大，以至于完全不能呈现\n",
    "##“减小-震荡”的趋势，呈现“发散震荡”的规律。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
