{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.复习上课内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is independent assumption in Naive bayes ?\n",
    "\n",
    "样本中各维度特征 $$ x_1, x_2, x_3, x_4, ... ,x_n $$相互独立，即: $$ P(x_1, x_2, x_3, x_4, ... ,x_n|y_k) = \\prod_{i=1}^n P(x_i|y_k) $$\n",
    "  \n",
    "2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?\n",
    "\n",
    "根据贝叶斯公式，$$ P(class|x) = P(x|class)*P(class) $$\n",
    "MAP: 对于x属于哪一类，即: 求$$argmax(P(class|x)) = argmax(P(x|class)*P(class))$$\n",
    "ML: 假设先验概率P(class)为均匀分布，那么$$argmax(P(class|x)) = argmax(P(x|class) $$\n",
    "对于x属于哪一类预估的标准是：该类使得出现x的概率最大。\n",
    "\n",
    "\n",
    "3. What is support vector in SVM?\n",
    "\n",
    "最靠近最优分类超平面两侧的样本点（最优超平面对应的两侧分类面穿过的点），为支持向量。\n",
    "\n",
    "\n",
    "4. What is the intuition behind SVM ?\n",
    "\n",
    "在所有样本点中找到一个分类超平面，使所有样本点（包括正反分类）距离超平面尽可能远。\n",
    "\n",
    "\n",
    "5. Shortly describ what 'random' means in random forest ?\n",
    "\n",
    "随机取得样本、随机选择某些特征。\n",
    "\n",
    "\n",
    "6. What cariterion does XGBoost use to find the best split point in a tree ?\n",
    "\n",
    "使得增益Gain函数的值最大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practial part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Temp\\jieba.cache\n",
      "Loading model cost 1.010 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "###对新闻语料进行分词处理，生成训练和测试数据集；新华社新闻Label为“Y”，非新华社新闻为“N”\n",
    "import jieba\n",
    "#import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def dataset(file_path):\n",
    "    x_trainings = []\n",
    "    y_labels = []\n",
    "    csv = pd.read_csv(file_path, encoding='gb18030')\n",
    "    for line in csv.values:\n",
    "        x_trainings.append(line[3])\n",
    "        if line[2] == '新华社':\n",
    "            y_labels.append('Y')\n",
    "        else:\n",
    "            y_labels.append('N')\n",
    "    return x_trainings, y_labels\n",
    "\n",
    "file_path = r'C:\\Users\\dell\\Desktop\\others\\NLP-lecture10\\sqlResult_1558435.csv'\n",
    "x_trainings, y_labels = dataset(file_path)[0], dataset(file_path)[1]\n",
    "\n",
    "stoplist = {}.fromkeys([line.strip() for line in open('stopwords.txt', encoding='utf-8')])\n",
    "\n",
    "def fenci(trainning_data):\n",
    "    #trainning_data = re.findall(r'[\\u4e00-\\u9fa5]',trainning_data)\n",
    "    seg = [word for word in jieba.cut(str(trainning_data)) if word not in stoplist]\n",
    "    text_string = ' '.join(seg)\n",
    "    return text_string\n",
    "\n",
    "def x_trainning_fenci(x_trainning):\n",
    "    X_trainning = []\n",
    "    for text in x_trainning:\n",
    "        X_trainning.append(fenci(str(text)))\n",
    "    return X_trainning\n",
    "\n",
    "X_trainning = x_trainning_fenci(x_trainings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trainning, y_labels, random_state=1)\n",
    "\n",
    "#print(X_train[:10])\n",
    "#print(y_train[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7499570023e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mDTC_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mDTC_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(DTC_classifier.score(TV.transform(X_test), y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDTC_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TV' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DTC_classifier = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=10)\n",
    "DTC_classifier.fit(TV.transform(X_train), y_train)\n",
    "#print(DTC_classifier.score(TV.transform(X_test), y_test))\n",
    "y_predict = DTC_classifier.predict(TV.transform(X_test))\n",
    "results = classification_report(y_test, y_predict, digits=4)\n",
    "#dict_1 = results.to_list(orient='records')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', '１２'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###使用sklearn-TfidfVectorizer进行text向量转换，并建立classifier模型\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "stoplist = {}.fromkeys([line.strip() for line in open('stopwords.txt', encoding='utf-8')])\n",
    "TV = TfidfVectorizer(stop_words=stoplist, max_features=10000, smooth_idf=True, lowercase = False)\n",
    "TV.fit(X_trainning)\n",
    "X_train_vector = TV.transform(X_train)#.toarray()\n",
    "X_test_vector = TV.transform(X_test)#.toarray()\n",
    "\n",
    "def bi_classification(x_trainning, y_trainning, x_test, y_test):\n",
    "    models=[]\n",
    "    models_params = {}\n",
    "    models.append(('MultinomialNB', MultinomialNB()))\n",
    "    #models.append(('GaussianNB', GaussianNB()))\n",
    "    models.append(('BernoulliNB', BernoulliNB()))\n",
    "    models.append(('KNN', KNeighborsClassifier(n_neighbors=6)))\n",
    "    models.append(('LogisticRegre', LogisticRegression(penalty='l1', tol=0.01)))\n",
    "    models.append(('SVM', svm.SVC(kernel='linear', probability=True, C=1000)))\n",
    "    models.append(('Decision_Tree', DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=100)))\n",
    "    models.append(('RandomForest', RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=100)))\n",
    "    models.append(('xgbc', XGBClassifier()))\n",
    "    \n",
    "    for classifier_method, classifier in models: \n",
    "        model_params = []\n",
    "        start_time = datetime.datetime.now()\n",
    "        #if classifier_method == 'GaussianNB':\n",
    "            #classifier.fit(x_trainning.toarray(), y_trainning)\n",
    "        #else:\n",
    "        classifier.fit(x_trainning, y_trainning)\n",
    "        end_time = datetime.datetime.now()\n",
    "        run_time = (end_time - start_time).microseconds\n",
    "        y_predict = classifier.predict(x_test)\n",
    "        Accuracy = accuracy_score(y_test, y_predict)\n",
    "        precision = precision_score(y_test, y_predict, pos_label='Y')\n",
    "        recall = recall_score(y_test, y_predict, pos_label='Y')\n",
    "        F1_score = f1_score(y_test, y_predict, pos_label='Y')\n",
    "        model_params.extend([Accuracy, precision, recall, F1_score, run_time])\n",
    "        models_params[classifier_method] = model_params\n",
    "        \n",
    "    return models_params\n",
    "\n",
    "models_results = bi_classification(X_train_vector, y_train, X_test_vector, y_test)\n",
    "#print(bi_classification(X_train_vector, y_train, X_test_vector, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Y_Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Run_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.911574</td>\n",
       "      <td>0.975876</td>\n",
       "      <td>0.922214</td>\n",
       "      <td>0.948286</td>\n",
       "      <td>75004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.810472</td>\n",
       "      <td>0.940418</td>\n",
       "      <td>0.837471</td>\n",
       "      <td>0.885964</td>\n",
       "      <td>125007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.919297</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.973343</td>\n",
       "      <td>0.954967</td>\n",
       "      <td>42002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegre</th>\n",
       "      <td>0.988662</td>\n",
       "      <td>0.989673</td>\n",
       "      <td>0.997512</td>\n",
       "      <td>0.993577</td>\n",
       "      <td>456312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.974691</td>\n",
       "      <td>0.983079</td>\n",
       "      <td>0.988220</td>\n",
       "      <td>0.985643</td>\n",
       "      <td>477593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision_Tree</th>\n",
       "      <td>0.984734</td>\n",
       "      <td>0.988096</td>\n",
       "      <td>0.994618</td>\n",
       "      <td>0.991346</td>\n",
       "      <td>151610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.971298</td>\n",
       "      <td>0.969307</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.983922</td>\n",
       "      <td>394484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgbc</th>\n",
       "      <td>0.982636</td>\n",
       "      <td>0.990398</td>\n",
       "      <td>0.989845</td>\n",
       "      <td>0.990122</td>\n",
       "      <td>725018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Y_Precision    Recall  F1_score  Run_time\n",
       "MultinomialNB  0.911574     0.975876  0.922214  0.948286     75004\n",
       "BernoulliNB    0.810472     0.940418  0.837471  0.885964    125007\n",
       "KNN            0.919297     0.937271  0.973343  0.954967     42002\n",
       "LogisticRegre  0.988662     0.989673  0.997512  0.993577    456312\n",
       "SVM            0.974691     0.983079  0.988220  0.985643    477593\n",
       "Decision_Tree  0.984734     0.988096  0.994618  0.991346    151610\n",
       "RandomForest   0.971298     0.969307  0.998985  0.983922    394484\n",
       "xgbc           0.982636     0.990398  0.989845  0.990122    725018"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###对比各classifier主要评价参数和运行时间\n",
    "pd.DataFrame.from_dict(models_results, orient='index', columns=['Accuracy', 'Y_Precision', 'Recall', 'F1_score', 'Run_time'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## SVM, RandomForest，xgbc模型需要多次遍历所有样本点，因此运行时间较长（与SVM 惩罚参数C，RandomForest树的最大深度和树的数量等参数也有关系）；但SVM, RandomForest，xgbc模型的Accuracy, Y_Precision,Recall，F1_score较高(当然这些参数与model parameters也有较大的关系)；LogisticRegression 的运行时间以及模型评价参数均表现很好，但模型的泛化能力不足，容易过拟合；朴素贝叶斯和KNN分类器在运行消耗时间方面表现好，但准确率相对欠缺；综合上述分类器表现，Decision_Tree在本新闻分类模型中综合表更好（需要优化最大数深度等参数）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
